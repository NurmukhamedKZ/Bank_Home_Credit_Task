"""
–ü—Ä–∏–º–µ—Ä—ã –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞
–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import List, Dict
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats


def load_experiment_results(result_file: str | Path) -> Dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏–∑ JSON"""
    with open(result_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def compare_two_experiments(result1: Dict, result2: Dict) -> pd.DataFrame:
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    
    Returns:
        DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏
    """
    metrics1 = result1['metrics_summary']['mean']
    metrics2 = result2['metrics_summary']['mean']
    
    comparison = []
    for metric in metrics1.keys():
        val1 = metrics1[metric]
        val2 = metrics2[metric]
        improvement = ((val2 - val1) / val1 * 100) if val1 > 0 else 0
        
        comparison.append({
            'metric': metric,
            f"{result1['config']['name']}": val1,
            f"{result2['config']['name']}": val2,
            'improvement_%': improvement
        })
    
    df = pd.DataFrame(comparison)
    return df


def plot_metrics_comparison(results: List[Dict], output_path: str = None):
    """
    –°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
    """
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data = []
    for result in results:
        row = {'experiment': result['config']['name']}
        row.update(result['metrics_summary']['mean'])
        data.append(row)
    
    df = pd.DataFrame(data)
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('Comparison of Search Quality Metrics', fontsize=16, fontweight='bold')
    
    metrics = ['precision@5', 'recall@10', 'map', 'mrr', 'ndcg@5', 'f1@5']
    
    for idx, metric in enumerate(metrics):
        if metric not in df.columns:
            continue
        
        row = idx // 3
        col = idx % 3
        ax = axes[row, col]
        
        # Bar plot
        df.plot(x='experiment', y=metric, kind='bar', ax=ax, legend=False)
        ax.set_title(metric.upper(), fontweight='bold')
        ax.set_xlabel('')
        ax.set_ylabel('Score')
        ax.set_ylim([0, 1.0])
        ax.grid(axis='y', alpha=0.3)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –±–∞—Ä–∞—Ö
        for container in ax.containers:
            ax.bar_label(container, fmt='%.3f', padding=3)
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    else:
        plt.show()
    
    return fig


def plot_confusion_matrix(results: List[Dict], output_path: str = None):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –ø—É—Ç–∞–Ω–∏—Ü—ã
    
    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    # –°—Ç—Ä–æ–∏–º –º–∞—Ç—Ä–∏—Ü—É
    from evaluate_search import CVSearchEvaluator
    from service.parse_pdf import CVParser
    
    parser = CVParser()
    evaluator = CVSearchEvaluator(parser)
    
    confusion = evaluator.generate_confusion_matrix(results)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        confusion,
        annot=True,
        fmt='g',
        cmap='YlOrRd',
        cbar_kws={'label': 'Number of CVs retrieved'}
    )
    plt.title('Confusion Matrix: Retrieved CV Types per Vacancy', fontweight='bold', pad=20)
    plt.xlabel('Retrieved CV Type', fontweight='bold')
    plt.ylabel('Vacancy Type', fontweight='bold')
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"üìä –ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
    else:
        plt.show()


def statistical_significance_test(results1: List[float], results2: List[float]) -> Dict:
    """
    –¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–∏–π
    
    Args:
        results1: –ú–µ—Ç—Ä–∏–∫–∏ –ø–µ—Ä–≤–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º)
        results2: –ú–µ—Ç—Ä–∏–∫–∏ –≤—Ç–æ—Ä–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º)
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã t-—Ç–µ—Å—Ç–∞
    """
    # Paired t-test (—Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –Ω–∞ –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ –≤–∞–∫–∞–Ω—Å–∏—è—Ö)
    t_stat, p_value = stats.ttest_rel(results1, results2)
    
    # Cohen's d (—Ä–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞)
    mean_diff = np.mean(results2) - np.mean(results1)
    pooled_std = np.sqrt((np.std(results1)**2 + np.std(results2)**2) / 2)
    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0
    
    return {
        't_statistic': t_stat,
        'p_value': p_value,
        'significant': p_value < 0.05,
        'cohens_d': cohens_d,
        'effect_size': (
            'small' if abs(cohens_d) < 0.5 else
            'medium' if abs(cohens_d) < 0.8 else
            'large'
        ),
        'mean_improvement': mean_diff
    }


def analyze_per_vacancy_performance(results: List[Dict]) -> pd.DataFrame:
    """
    –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
    
    Returns:
        DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
    """
    rows = []
    
    for result in results:
        for vacancy_result in result['detailed_results']:
            row = {
                'experiment': result['config']['name'],
                'vacancy': vacancy_result['vacancy']
            }
            row.update(vacancy_result['metrics'])
            rows.append(row)
    
    df = pd.DataFrame(rows)
    return df


def find_difficult_vacancies(per_vacancy_df: pd.DataFrame, metric: str = 'map') -> pd.DataFrame:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –≤–∞–∫–∞–Ω—Å–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä—É–¥–Ω–µ–µ –≤—Å–µ–≥–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å
    
    Args:
        per_vacancy_df: DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
        metric: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        
    Returns:
        DataFrame —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    """
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º –∏ —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –º–µ—Ç—Ä–∏–∫—É
    difficulty = per_vacancy_df.groupby('vacancy')[metric].mean().reset_index()
    difficulty['difficulty_score'] = 1 - difficulty[metric]  # –ß–µ–º –Ω–∏–∂–µ –º–µ—Ç—Ä–∏–∫–∞, —Ç–µ–º —Å–ª–æ–∂–Ω–µ–µ
    difficulty = difficulty.sort_values('difficulty_score', ascending=False)
    
    return difficulty


def plot_per_vacancy_performance(per_vacancy_df: pd.DataFrame, output_path: str = None):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # MAP –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
    map_by_vacancy = per_vacancy_df.pivot_table(
        values='map',
        index='vacancy',
        columns='experiment'
    )
    
    map_by_vacancy.plot(kind='bar', ax=axes[0])
    axes[0].set_title('MAP by Vacancy', fontweight='bold')
    axes[0].set_xlabel('Vacancy')
    axes[0].set_ylabel('MAP Score')
    axes[0].legend(title='Experiment')
    axes[0].grid(axis='y', alpha=0.3)
    
    # Precision@5 –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
    p5_by_vacancy = per_vacancy_df.pivot_table(
        values='precision@5',
        index='vacancy',
        columns='experiment'
    )
    
    p5_by_vacancy.plot(kind='bar', ax=axes[1])
    axes[1].set_title('Precision@5 by Vacancy', fontweight='bold')
    axes[1].set_xlabel('Vacancy')
    axes[1].set_ylabel('Precision@5 Score')
    axes[1].legend(title='Experiment')
    axes[1].grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    else:
        plt.show()


def generate_full_report(
    results: List[Dict],
    output_dir: str | Path = "analysis_reports"
):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        output_dir: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    print("\n" + "="*70)
    print("üìä –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–õ–ù–û–ì–û –û–¢–ß–ï–¢–ê")
    print("="*70 + "\n")
    
    # 1. –û–±—â–µ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    print("1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")
    plot_metrics_comparison(
        results,
        output_path=output_dir / "metrics_comparison.png"
    )
    
    # 2. –ê–Ω–∞–ª–∏–∑ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
    print("2. –ê–Ω–∞–ª–∏–∑ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º...")
    per_vacancy_df = analyze_per_vacancy_performance(results)
    per_vacancy_df.to_csv(output_dir / "per_vacancy_metrics.csv", index=False)
    
    plot_per_vacancy_performance(
        per_vacancy_df,
        output_path=output_dir / "per_vacancy_performance.png"
    )
    
    # 3. –°–ª–æ–∂–Ω–æ—Å—Ç—å –≤–∞–∫–∞–Ω—Å–∏–π
    print("3. –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤–∞–∫–∞–Ω—Å–∏–π...")
    difficulty = find_difficult_vacancies(per_vacancy_df)
    difficulty.to_csv(output_dir / "vacancy_difficulty.csv", index=False)
    
    print("\nüìã –°–∞–º—ã–µ —Å–ª–æ–∂–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏:")
    print(difficulty.head().to_string(index=False))
    
    # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å (–µ—Å–ª–∏ >= 2 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)
    if len(results) >= 2:
        print("\n4. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å –∫–∞–∂–¥—ã–º –ø–æ—Å–ª–µ–¥—É—é—â–∏–º
        baseline = results[0]
        baseline_maps = [v['metrics']['map'] for v in baseline['detailed_results']]
        
        sig_tests = []
        for i, result in enumerate(results[1:], 1):
            result_maps = [v['metrics']['map'] for v in result['detailed_results']]
            test_result = statistical_significance_test(baseline_maps, result_maps)
            
            sig_tests.append({
                'comparison': f"{baseline['config']['name']} vs {result['config']['name']}",
                'p_value': test_result['p_value'],
                'significant': test_result['significant'],
                'mean_improvement': test_result['mean_improvement'],
                'cohens_d': test_result['cohens_d'],
                'effect_size': test_result['effect_size']
            })
        
        sig_df = pd.DataFrame(sig_tests)
        sig_df.to_csv(output_dir / "statistical_tests.csv", index=False)
        
        print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤:")
        print(sig_df.to_string(index=False))
    
    # 5. –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print("\n5. –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞...")
    summary_rows = []
    for result in results:
        row = {
            'Experiment': result['config']['name'],
            'Description': result['config']['description']
        }
        metrics = result['metrics_summary']['mean']
        row.update({
            'MAP': f"{metrics['map']:.3f}",
            'P@5': f"{metrics['precision@5']:.3f}",
            'R@10': f"{metrics['recall@10']:.3f}",
            'MRR': f"{metrics['mrr']:.3f}"
        })
        summary_rows.append(row)
    
    summary_df = pd.DataFrame(summary_rows)
    summary_df.to_csv(output_dir / "summary.csv", index=False)
    
    print("\n" + "="*70)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê")
    print("="*70)
    print(summary_df.to_string(index=False))
    
    print("\n" + "="*70)
    print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_dir}")
    print("="*70 + "\n")
    
    return output_dir


# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def example_basic_analysis():
    """–ü—Ä–∏–º–µ—Ä –±–∞–∑–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    results_dir = Path("app/experiments/results")
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    json_files = sorted(results_dir.glob("*.json"), key=lambda x: x.stat().st_mtime)
    
    if len(json_files) < 2:
        print("‚ö†Ô∏è  –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2
    result1 = load_experiment_results(json_files[-2])
    result2 = load_experiment_results(json_files[-1])
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º
    comparison = compare_two_experiments(result1, result2)
    print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:")
    print(comparison.to_string(index=False))
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç
    maps1 = [v['metrics']['map'] for v in result1['detailed_results']]
    maps2 = [v['metrics']['map'] for v in result2['detailed_results']]
    
    test = statistical_significance_test(maps1, maps2)
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å:")
    print(f"   p-value: {test['p_value']:.4f}")
    print(f"   –ó–Ω–∞—á–∏–º–æ: {'–î–∞ ‚úÖ' if test['significant'] else '–ù–µ—Ç ‚ùå'}")
    print(f"   Effect size: {test['effect_size']}")


def example_full_analysis():
    """–ü—Ä–∏–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    results_dir = Path("app/experiments/results")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    all_results = []
    for json_file in results_dir.glob("*.json"):
        if json_file.stem.startswith("comparison"):
            continue
        all_results.append(load_experiment_results(json_file))
    
    if not all_results:
        print("‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_results)} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç
    report_dir = generate_full_report(all_results)
    
    print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –°–º–æ—Ç—Ä–∏—Ç–µ: {report_dir}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "full":
        example_full_analysis()
    else:
        example_basic_analysis()

"""
FastAPI —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.
"""

from typing import Optional
from pathlib import Path
from fastapi import APIRouter, HTTPException, Depends

from app.models.api import (
    SearchRequest,
    SearchResponse,
    HealthResponse,
    APIInfoResponse,
    SearchWithLLMResponse,
    CandidateWithLLMAnalysis,
    LLMAnalysisResult,
    MLClassifierRequest,
    CandidateMLResult,
    MLClassifierResponse,
    WorkExperienceResponse,
)
from app.models.cv import CVOutput, WorkExperience
from app.services.cv_parser import CVParser
from app.services.search import search_candidates
from app.services.llm_analyze import LLMAnalyzer
from app.services.ml_classifier import MLClassifier


router = APIRouter()

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è CVParser (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –≤ lifespan)
_cv_parser: Optional[CVParser] = None


def get_cv_parser() -> CVParser:
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è CVParser"""
    if _cv_parser is None:
        raise HTTPException(status_code=503, detail="CVParser –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    return _cv_parser


def set_cv_parser(parser: CVParser):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ CVParser"""
    global _cv_parser
    _cv_parser = parser


def clear_cv_parser():
    """–û—á–∏—Å—Ç–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ CVParser"""
    global _cv_parser
    _cv_parser = None


@router.get("/", response_model=APIInfoResponse, tags=["Info"])
async def root():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± API"""
    return APIInfoResponse(
        name="CV Search API",
        version="1.0.0",
        description="API –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É –≤–∞–∫–∞–Ω—Å–∏–∏",
        endpoints={
            "POST /search": "–ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (Vector Search)",
            "POST /search/with-llm-analysis": "–ü–æ–∏—Å–∫ —Å LLM –∞–Ω–∞–ª–∏–∑–æ–º —Ç–æ–ø-5",
            "POST /search/ml-classifier": "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (TF-IDF + Logistic)",
            "GET /health": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞",
            "GET /": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± API"
        }
    )


@router.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check(parser: CVParser = Depends(get_cv_parser)):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        collection_info = parser.qdrant_client.get_collection(parser.collection_name)
        
        return HealthResponse(
            status="healthy",
            collection=parser.collection_name,
            documents_count=collection_info.points_count,
            sparse_fitted=parser._sparse_fitted,
            sparse_method=parser.sparse_method
        )
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {str(e)}")


@router.post("/search", response_model=SearchResponse, tags=["Search"])
async def search(request: SearchRequest, parser: CVParser = Depends(get_cv_parser)):
    """
    –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É –≤–∞–∫–∞–Ω—Å–∏–∏
    
    - **vacancy_text**: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏, –Ω–∞–≤—ã–∫–∏)
    - **search_mode**: –†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞
        - `dense` - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Voyage AI embeddings
        - `sparse` - keyword –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ TF-IDF
        - `hybrid` - –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –æ–±–æ–∏—Ö –º–µ—Ç–æ–¥–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
    - **top_k**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ (1-50)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞
    actual_mode = request.search_mode
    if actual_mode in ["sparse", "hybrid"] and not parser._sparse_fitted:
        actual_mode = "dense" if request.search_mode == "hybrid" else request.search_mode
    
    candidates = search_candidates(
        parser=parser,
        query_text=request.vacancy_text,
        top_k=request.top_k,
        search_mode=request.search_mode
    )
    
    return SearchResponse(
        query_preview=request.vacancy_text[:100] + "..." if len(request.vacancy_text) > 100 else request.vacancy_text,
        search_mode=actual_mode,
        results_count=len(candidates),
        candidates=candidates
    )


@router.post("/search/with-llm-analysis", response_model=SearchWithLLMResponse, tags=["Search"])
async def search_with_llm_analysis(
    request: SearchRequest,
    parser: CVParser = Depends(get_cv_parser)
):
    """
    –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å LLM –∞–Ω–∞–ª–∏–∑–æ–º —Ç–æ–ø-5
    
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –¥–≤–∞ —ç—Ç–∞–ø–∞:
    1. **–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫** - –Ω–∞—Ö–æ–¥–∏—Ç —Ç–æ–ø-K –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ Qdrant (dense/sparse/hybrid)
    2. **LLM –∞–Ω–∞–ª–∏–∑** - –¥–ª—è —Ç–æ–ø-5 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É —á–µ—Ä–µ–∑ GPT-4
    
    LLM –∞–Ω–∞–ª–∏–∑ –≤–∫–ª—é—á–∞–µ—Ç:
    - –û—Ü–µ–Ω–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0-1) –Ω–∞ –æ—Å–Ω–æ–≤–µ 4 –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
    - –°–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
    - –ö–ª—é—á–µ–≤—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏
    - –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ
    
    **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
    - **vacancy_text**: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏
    - **search_mode**: dense/sparse/hybrid (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è hybrid)
    - **top_k**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (1-50)
    
    **‚ö†Ô∏è –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: LLM –∞–Ω–∞–ª–∏–∑ –∑–∞–Ω–∏–º–∞–µ—Ç ~3-5 —Å–µ–∫—É–Ω–¥ –Ω–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞
    actual_mode = request.search_mode
    if actual_mode in ["sparse", "hybrid"] and not parser._sparse_fitted:
        actual_mode = "dense" if request.search_mode == "hybrid" else request.search_mode
    
    # –®–∞–≥ 1: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    print(f"üîç –ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ {actual_mode}...")
    candidates = search_candidates(
        parser=parser,
        query_text=request.vacancy_text,
        top_k=request.top_k,
        search_mode=request.search_mode
    )
    
    if not candidates:
        return SearchWithLLMResponse(
            query_preview=request.vacancy_text[:100] + "..." if len(request.vacancy_text) > 100 else request.vacancy_text,
            search_mode=actual_mode,
            results_count=0,
            llm_analyzed_count=0,
            candidates=[]
        )
    
    # –®–∞–≥ 2: LLM –∞–Ω–∞–ª–∏–∑ —Ç–æ–ø-5 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    top_n = min(5, len(candidates))
    print(f"ü§ñ LLM –∞–Ω–∞–ª–∏–∑ —Ç–æ–ø-{top_n} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤...")
    
    analyzer = LLMAnalyzer(model="gpt-4o", temperature=0.3)
    
    candidates_with_llm = []
    
    for i, candidate in enumerate(candidates, 1):
        # –î–ª—è —Ç–æ–ø-5 –¥–æ–±–∞–≤–ª—è–µ–º LLM –∞–Ω–∞–ª–∏–∑
        if i <= top_n:
            try:
                print(f"   [{i}/{top_n}] –ê–Ω–∞–ª–∏–∑: {candidate.full_name}...")
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º CandidateResult –≤ CVOutput –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                cv_data = CVOutput(
                    full_name=candidate.full_name,
                    email=candidate.email,
                    phone=candidate.phone,
                    location=candidate.location,
                    summary=candidate.summary,
                    total_experience_months=candidate.total_experience_months,
                    work_history=[
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º WorkExperienceResponse –æ–±—Ä–∞—Ç–Ω–æ –≤ WorkExperience
                        WorkExperience(
                            role=w.role,
                            company=w.company,
                            start_date=w.start_date,
                            end_date=w.end_date,
                            description=w.description,
                            technologies=w.technologies
                        )
                        for w in candidate.work_history
                    ],
                    education=[],  # –£–ø—Ä–æ—â–∞–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    skills=candidate.skills,
                    languages=candidate.languages
                )
                
                # LLM –∞–Ω–∞–ª–∏–∑
                llm_result = analyzer.analyze_match(cv_data, request.vacancy_text)
                
                # –°–æ–∑–¥–∞–µ–º CandidateWithLLMAnalysis
                candidate_with_llm = CandidateWithLLMAnalysis(
                    **candidate.dict(),
                    llm_analysis=LLMAnalysisResult(
                        relevance_score=llm_result.relevance_score,
                        overall_assessment=llm_result.overall_assessment,
                        summary=llm_result.summary,
                        strengths=llm_result.strengths,
                        weaknesses=llm_result.weaknesses,
                        key_matches=llm_result.key_matches,
                        missing_requirements=llm_result.missing_requirements,
                        recommendation=llm_result.recommendation,
                        reasoning=llm_result.reasoning
                    )
                )
                
                print(f"      ‚úÖ LLM Score: {llm_result.relevance_score:.3f}")
                
            except Exception as e:
                print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞: {e}")
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ - –¥–æ–±–∞–≤–ª—è–µ–º –±–µ–∑ LLM –∞–Ω–∞–ª–∏–∑–∞
                candidate_with_llm = CandidateWithLLMAnalysis(**candidate.dict())
        else:
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –±–µ–∑ LLM –∞–Ω–∞–ª–∏–∑–∞
            candidate_with_llm = CandidateWithLLMAnalysis(**candidate.dict())
        
        candidates_with_llm.append(candidate_with_llm)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
    llm_analyzed = sum(1 for c in candidates_with_llm if c.llm_analysis is not None)
    
    print(f"‚úÖ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(candidates_with_llm)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, {llm_analyzed} —Å LLM –∞–Ω–∞–ª–∏–∑–æ–º")
    
    return SearchWithLLMResponse(
        query_preview=request.vacancy_text[:100] + "..." if len(request.vacancy_text) > 100 else request.vacancy_text,
        search_mode=actual_mode,
        results_count=len(candidates_with_llm),
        llm_analyzed_count=llm_analyzed,
        candidates=candidates_with_llm
    )


@router.post("/search/ml-classifier", response_model=MLClassifierResponse, tags=["Search"])
async def search_ml_classifier(
    request: MLClassifierRequest,
    parser: CVParser = Depends(get_cv_parser)
):
    """
    –ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (TF-IDF + Logistic Regression)
    
    **Supervised learning –ø–æ–¥—Ö–æ–¥:**
    1. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–π ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ TF-IDF —Ñ–∏—á–∞—Ö
    2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ CV –≤ –±–∞–∑–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    3. –†–∞–Ω–∂–∏—Ä—É–µ—Ç –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-K
    
    **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
    - ‚ö° –ë—ã—Å—Ç—Ä–æ (~1-2 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è –≤—Å–µ–π –±–∞–∑—ã)
    - üìä –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (–æ–±—É—á–µ–Ω –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
    - üéØ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ (–º–æ–∂–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å feature importance)
    - üí∞ –ë–µ—Å–ø–ª–∞—Ç–Ω–æ (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç API)
    
    **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
    - **vacancy_text**: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏
    - **top_k**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ (1-50)
    - **threshold**: –ü–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (0.0-1.0, default=0.5)
    
    **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –¢—Ä–µ–±—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ `data/models/ml_classifier_evaluation.pkl`
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    model_path = Path("data/models/ml_classifier_evaluation.pkl")
    
    if not model_path.exists():
        raise HTTPException(
            status_code=503,
            detail="ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python -m app.scripts.evaluate_ml_classifier"
        )
    
    try:
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        classifier = MLClassifier.load(model_path)
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        raise HTTPException(
            status_code=503,
            detail=f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}"
        )
    
    print(f"üîç ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤...")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ CV –∏–∑ Qdrant
    try:
        scroll_result = parser.qdrant_client.scroll(
            collection_name=parser.collection_name,
            limit=1000,
            with_payload=True,
            with_vectors=False
        )
        all_points = scroll_result[0]
        print(f"   üìä –ù–∞–π–¥–µ–Ω–æ CV –≤ –±–∞–∑–µ: {len(all_points)}")
    except Exception as e:
        raise HTTPException(
            status_code=503,
            detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è CV –∏–∑ Qdrant: {str(e)}"
        )
    
    if not all_points:
        return MLClassifierResponse(
            query_preview=request.vacancy_text[:100] + "..." if len(request.vacancy_text) > 100 else request.vacancy_text,
            results_count=0,
            threshold=request.threshold,
            relevant_count=0,
            candidates=[]
        )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ CV
    print(f"   ü§ñ ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è {len(all_points)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤...")
    
    candidates_with_scores = []
    
    for point in all_points:
        payload = point.payload
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç CV –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        cv_text = payload.get('full_content', '')
        
        if not cv_text:
            # Fallback: —Å–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            cv_text = f"{payload.get('summary', '')} {' '.join(payload.get('skills', []))}"
        
        try:
            # ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            ml_probability = classifier.predict_proba(request.vacancy_text, cv_text)
            ml_prediction = 1 if ml_probability >= request.threshold else 0
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º work_history
            work_history = []
            for work in payload.get('work_history', []):
                work_history.append(WorkExperienceResponse(
                    role=work.get('role', ''),
                    company=work.get('company', ''),
                    start_date=work.get('start_date', ''),
                    end_date=work.get('end_date', ''),
                    description=work.get('description', ''),
                    technologies=work.get('technologies', [])
                ))
            
            candidate = CandidateMLResult(
                rank=0,  # –£—Å—Ç–∞–Ω–æ–≤–∏–º –ø–æ–∑–∂–µ –ø–æ—Å–ª–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
                score=ml_probability,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ML –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–∞–∫ score
                full_name=payload.get('full_name', 'Unknown'),
                email=payload.get('email'),
                phone=payload.get('phone'),
                location=payload.get('location', []),
                summary=payload.get('summary', ''),
                skills=payload.get('skills', []),
                total_experience_months=payload.get('total_experience_months', 0),
                work_history=work_history,
                languages=payload.get('languages', []),
                links=payload.get('links', []),
                source_file=payload.get('source_file'),
                ml_probability=ml_probability,
                ml_prediction=ml_prediction
            )
            
            candidates_with_scores.append(candidate)
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {payload.get('full_name', 'Unknown')}: {e}")
            continue
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ML –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    candidates_with_scores.sort(key=lambda x: x.ml_probability, reverse=True)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞–Ω–≥–∏
    for rank, candidate in enumerate(candidates_with_scores[:request.top_k], 1):
        candidate.rank = rank
    
    # –ë–µ—Ä–µ–º —Ç–æ–ø-K
    top_candidates = candidates_with_scores[:request.top_k]
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
    relevant_count = sum(1 for c in top_candidates if c.ml_prediction == 1)
    
    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(top_candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, {relevant_count} –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ {request.threshold}")
    
    return MLClassifierResponse(
        query_preview=request.vacancy_text[:100] + "..." if len(request.vacancy_text) > 100 else request.vacancy_text,
        results_count=len(top_candidates),
        threshold=request.threshold,
        relevant_count=relevant_count,
        candidates=top_candidates
    )

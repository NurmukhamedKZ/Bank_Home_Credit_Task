"""
–ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ–∏—Å–∫–∞
–ü–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –Ω–∞—Ö–æ–¥–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ
"""

from pathlib import Path
from typing import Dict, List, Any, Optional
import json
import pandas as pd
from datetime import datetime
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –ø–∞–ø–∫–µ
sys.path.append(str(Path(__file__).parent.parent))

from service.parse_pdf import CVParser
from evaluate_search import CVSearchEvaluator


class ExperimentConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
    
    def __init__(
        self,
        name: str,
        description: str,
        dense_model: str = "voyage-4-large",
        dense_output_dim: int = 1024,
        tfidf_max_features: int = 10000,
        tfidf_ngram_range: tuple = (1, 2),
        tfidf_min_df: int = 1,
        system_prompt: Optional[str] = None,
        collection_name: str = "CVs_experiment"
    ):
        """
        Args:
            name: –ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
            description: –û–ø–∏—Å–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
            dense_model: –ú–æ–¥–µ–ª—å –¥–ª—è dense embeddings
            dense_output_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å dense –≤–µ–∫—Ç–æ—Ä–æ–≤
            tfidf_max_features: –ú–∞–∫—Å–∏–º—É–º —Ñ–∏—á–µ–π –¥–ª—è TF-IDF
            tfidf_ngram_range: N-–≥—Ä–∞–º–º—ã –¥–ª—è TF-IDF
            tfidf_min_df: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è TF-IDF
            system_prompt: –ö–∞—Å—Ç–æ–º–Ω—ã–π system prompt –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ CV
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant
        """
        self.name = name
        self.description = description
        self.dense_model = dense_model
        self.dense_output_dim = dense_output_dim
        self.tfidf_max_features = tfidf_max_features
        self.tfidf_ngram_range = tfidf_ngram_range
        self.tfidf_min_df = tfidf_min_df
        self.system_prompt = system_prompt
        self.collection_name = collection_name
    
    def to_dict(self) -> Dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            'name': self.name,
            'description': self.description,
            'dense_model': self.dense_model,
            'dense_output_dim': self.dense_output_dim,
            'tfidf_max_features': self.tfidf_max_features,
            'tfidf_ngram_range': self.tfidf_ngram_range,
            'tfidf_min_df': self.tfidf_min_df,
            'system_prompt': self.system_prompt,
            'collection_name': self.collection_name
        }
    
    @classmethod
    def from_dict(cls, config_dict: Dict) -> 'ExperimentConfig':
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        return cls(**config_dict)
    
    @classmethod
    def from_json(cls, json_path: str | Path) -> 'ExperimentConfig':
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        with open(json_path, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
        return cls.from_dict(config_dict)


class ExperimentRunner:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    
    def __init__(self, experiments_dir: str | Path = "app/experiments"):
        """
        Args:
            experiments_dir: –ü–∞–ø–∫–∞ —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏
        """
        self.experiments_dir = Path(experiments_dir)
        self.results_dir = self.experiments_dir / "results"
        self.configs_dir = self.experiments_dir / "configs"
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
        self.results_dir.mkdir(exist_ok=True, parents=True)
        self.configs_dir.mkdir(exist_ok=True, parents=True)
        
        self.all_results = []
    
    def run_experiment(
        self,
        config: ExperimentConfig,
        cvs_to_process: Optional[List[Path]] = None,
        reuse_collection: bool = False
    ) -> Dict:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
            cvs_to_process: –°–ø–∏—Å–æ–∫ CV –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ)
            reuse_collection: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é (–Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å CV –∑–∞–Ω–æ–≤–æ)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        """
        print(f"\n{'='*70}")
        print(f"üß™ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: {config.name}")
        print(f"üìù {config.description}")
        print(f"{'='*70}\n")
        
        timestamp = datetime.now()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º parser —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        print("‚öôÔ∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CVParser...")
        parser = CVParser(
            collection_name=config.collection_name,
            dense_model_name=config.dense_model,
            dense_output_dim=config.dense_output_dim
        )
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π system prompt –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if config.system_prompt:
            print("üìù –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ system prompt...")
            parser.system_prompt = config.system_prompt
            from langchain_core.prompts import ChatPromptTemplate
            parser.prompt = ChatPromptTemplate.from_messages([
                ("system", parser.system_prompt),
                ("user", "Resume:\n\n{text}")
            ])
            parser.chain = parser.prompt | parser.structured_llm
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º TF-IDF
        from sklearn.feature_extraction.text import TfidfVectorizer
        parser.sparse_model = TfidfVectorizer(
            max_features=config.tfidf_max_features,
            ngram_range=config.tfidf_ngram_range,
            min_df=config.tfidf_min_df,
            sublinear_tf=True,
            lowercase=True,
            stop_words='english'
        )
        parser._tfidf_fitted = False
        parser._tfidf_corpus = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º CV –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not reuse_collection:
            print("\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ CV...")
            
            if cvs_to_process is None:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ CV –∏–∑ data/CVs
                cvs_folder = Path("data/CVs")
                cvs_to_process = list(cvs_folder.glob("*.pdf"))
            
            if not cvs_to_process:
                print("‚ö†Ô∏è  CV –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            else:
                for i, cv_path in enumerate(cvs_to_process, 1):
                    print(f"  [{i}/{len(cvs_to_process)}] {cv_path.name}...", end=' ')
                    try:
                        parser.process_cv(cv_path)
                        print("‚úÖ")
                    except Exception as e:
                        print(f"‚ùå {e}")
                
                # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º TF-IDF –Ω–∞ –≤—Å–µ–º –∫–æ—Ä–ø—É—Å–µ
                print("\nüîÑ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ TF-IDF –Ω–∞ –≤—Å–µ–º –∫–æ—Ä–ø—É—Å–µ...")
                parser.refit_tfidf()
        else:
            print("‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è")
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞
        print("\nüìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞...")
        evaluator = CVSearchEvaluator(parser)
        df, results = evaluator.evaluate_all(top_k=10)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        experiment_result = {
            'config': config.to_dict(),
            'timestamp': timestamp.isoformat(),
            'metrics_summary': {
                'mean': df[[col for col in df.columns if col not in ['vacancy', 'relevant_count']]].mean().to_dict(),
                'std': df[[col for col in df.columns if col not in ['vacancy', 'relevant_count']]].std().to_dict()
            },
            'per_vacancy_metrics': df.to_dict(orient='records'),
            'detailed_results': results
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        result_file = self.results_dir / f"{config.name}_{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ lists –¥–ª—è JSON
        serializable_result = self._make_serializable(experiment_result)
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_result, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {result_file}")
        
        self.all_results.append(experiment_result)
        
        return experiment_result
    
    def _make_serializable(self, obj):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç –≤ JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, set):
            return list(obj)
        elif isinstance(obj, tuple):
            return list(obj)
        else:
            return obj
    
    def run_multiple_experiments(
        self,
        configs: List[ExperimentConfig],
        cvs_to_process: Optional[List[Path]] = None
    ) -> pd.DataFrame:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        
        Args:
            configs: –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            cvs_to_process: CV –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            DataFrame —Å–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫
        """
        print(f"\n{'='*70}")
        print(f"üî¨ –ó–ê–ü–£–°–ö {len(configs)} –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í")
        print(f"{'='*70}\n")
        
        comparison_rows = []
        
        for i, config in enumerate(configs, 1):
            print(f"\n[{i}/{len(configs)}]")
            
            try:
                result = self.run_experiment(
                    config,
                    cvs_to_process=cvs_to_process,
                    reuse_collection=(i > 1)  # –ü–µ—Ä–≤—ã–π —Ä–∞–∑ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º, –ø–æ—Ç–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
                row = {
                    'experiment': config.name,
                    'description': config.description
                }
                row.update(result['metrics_summary']['mean'])
                comparison_rows.append(row)
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ {config.name}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        comparison_df = pd.DataFrame(comparison_rows)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        comparison_file = self.results_dir / f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        comparison_df.to_csv(comparison_file, index=False)
        
        print(f"\n{'='*70}")
        print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í")
        print(f"{'='*70}\n")
        print(comparison_df.to_string(index=False))
        
        print(f"\nüíæ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {comparison_file}")
        
        return comparison_df
    
    def create_default_configs(self) -> List[ExperimentConfig]:
        """–°–æ–∑–¥–∞–µ—Ç –Ω–∞–±–æ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
        configs = [
            # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            ExperimentConfig(
                name="baseline",
                description="–ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (Voyage-4-large, TF-IDF unigrams+bigrams)",
                dense_model="voyage-4-large",
                dense_output_dim=1024,
                tfidf_max_features=10000,
                tfidf_ngram_range=(1, 2),
                collection_name="CVs_baseline"
            ),
            
            # –ë–æ–ª—å—à–µ n-–≥—Ä–∞–º–º
            ExperimentConfig(
                name="trigrams",
                description="TF-IDF —Å tri-grams –¥–ª—è –ª—É—á—à–µ–≥–æ –∑–∞—Ö–≤–∞—Ç–∞ —Ñ—Ä–∞–∑",
                dense_model="voyage-4-large",
                dense_output_dim=1024,
                tfidf_max_features=15000,
                tfidf_ngram_range=(1, 3),
                collection_name="CVs_trigrams"
            ),
            
            # –ú–µ–Ω—å—à–µ —Ñ–∏—á–µ–π (–±—ã—Å—Ç—Ä–µ–µ)
            ExperimentConfig(
                name="lightweight",
                description="–û–±–ª–µ–≥—á–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –º–µ–Ω—å—à–µ —Ñ–∏—á–µ–π TF-IDF",
                dense_model="voyage-4-large",
                dense_output_dim=1024,
                tfidf_max_features=5000,
                tfidf_ngram_range=(1, 2),
                collection_name="CVs_lightweight"
            ),
            
            # –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            ExperimentConfig(
                name="detailed_prompt",
                description="–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏",
                dense_model="voyage-4-large",
                dense_output_dim=1024,
                tfidf_max_features=10000,
                tfidf_ngram_range=(1, 2),
                system_prompt="""
You are an expert technical recruiter and CV parser specializing in IT positions.
Your task is to extract structured data from the provided resume text.

CRITICAL FOCUS AREAS:
1. Technical Skills: Extract ALL programming languages, frameworks, tools, and technologies
2. Work Experience: Be precise with dates, calculate total months accurately
3. Projects: Capture specific technologies and achievements
4. Education: Include degrees, institutions, and graduation years

EXTRACTION RULES:
- For 'skills', extract both hard skills (technical) and important soft skills
- For 'work_history', split distinct roles even if same company
- In 'total_experience_months', sum ALL work durations carefully
- For technologies, be specific (e.g., "Python 3.10", "FastAPI", not just "Python")
- Extract version numbers when mentioned

QUALITY STANDARDS:
- Prefer explicit information over assumptions
- If field is missing, leave as None or empty list
- Maintain exact terminology from CV (don't paraphrase technical terms)
""",
                collection_name="CVs_detailed_prompt"
            )
        ]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        for config in configs:
            config_file = self.configs_dir / f"{config.name}.json"
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config.to_dict(), f, ensure_ascii=False, indent=2)
            print(f"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_file}")
        
        return configs


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    
    runner = ExperimentRunner()
    
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π...")
    configs = runner.create_default_configs()
    
    print(f"\nüìã –°–æ–∑–¥–∞–Ω–æ {len(configs)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π:")
    for config in configs:
        print(f"  ‚Ä¢ {config.name}: {config.description}")
    
    # –í—ã–±–∏—Ä–∞–µ–º –∫–∞–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å
    print("\n" + "="*70)
    print("–í—ã–±–µ—Ä–∏—Ç–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞:")
    print("  1. Baseline")
    print("  2. Trigrams")
    print("  3. Lightweight")
    print("  4. Detailed Prompt")
    print("  5. –í—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã")
    print("="*70)
    
    choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (–∏–ª–∏ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
    
    if choice == 'q':
        print("–í—ã—Ö–æ–¥.")
        return
    
    if choice == '5':
        selected_configs = configs
    else:
        try:
            idx = int(choice) - 1
            if 0 <= idx < len(configs):
                selected_configs = [configs[idx]]
            else:
                print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä!")
                return
        except ValueError:
            print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥!")
            return
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
    comparison_df = runner.run_multiple_experiments(selected_configs)
    
    print("\n‚úÖ –í—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
    
    return comparison_df


if __name__ == "__main__":
    main()

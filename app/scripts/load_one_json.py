#!/usr/bin/env python3
"""
Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ¸Ğ· JSON Ğ² Qdrant.

Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ±ĞµÑ€ĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ JSON Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸Ğ· Ğ¿Ğ°Ğ¿ĞºĞ¸ CV_JSONs,
ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² Qdrant.

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    python -m app.scripts.load_jsons_to_qdrant [--refit-sparse]
"""

import json
from pathlib import Path
import argparse
from typing import List

from app.services.cv_parser import CVParser
from app.models.cv import CVOutput, WorkExperience, Education
from app.core.config import QDRANT_COLLECTION_NAME


def json_to_cv_output(data: dict) -> CVOutput:
    """ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ JSON Ğ² CVOutput Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ"""
    
    # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ work_history
    work_history = []
    for work in data.get("work_history", []):
        work_history.append(WorkExperience(
            role=work.get("role", ""),
            company=work.get("company", ""),
            start_date=work.get("start_date"),
            end_date=work.get("end_date"),
            description=work.get("description"),
            technologies=work.get("technologies", [])
        ))
    
    # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ education
    education = []
    for edu in data.get("education", []):
        education.append(Education(
            institution=edu.get("institution", ""),
            degree=edu.get("degree"),
            year=edu.get("year")
        ))
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ CVOutput
    return CVOutput(
        full_name=data.get("full_name", "Unknown"),
        email=data.get("email"),
        phone=data.get("phone"),
        links=data.get("links", []),
        location=data.get("location", []),
        summary=data.get("summary", ""),
        total_experience_months=data.get("total_experience_months", 0),
        work_history=work_history,
        education=education,
        skills=data.get("skills", []),
        languages=data.get("languages", [])
    )


def load_json_files(json_folder: Path, needing_file: str) -> List[tuple[Path, dict]]:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ²ÑĞµÑ… JSON Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ğ°Ğ¿ĞºĞ¸"""
    json_files = []
    
    if not json_folder.exists():
        print(f"âŒ ĞŸĞ°Ğ¿ĞºĞ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°: {json_folder}")
        return json_files
    
    for json_file in sorted(json_folder.glob("*.json")):
        try:
            if needing_file != json_file.name:
                continue
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            json_files.append((json_file, data))
        except Exception as e:
            print(f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ {json_file.name}: {e}")
    
    return json_files


def main():
    parser = argparse.ArgumentParser(description="Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° JSON Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ² Qdrant")
    parser.add_argument(
        "--refit-sparse",
        action="store_true",
        help="ĞŸĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ sparse Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (BM25/TF-IDF) Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸"
    )
    args = parser.parse_args()
    

    needing_file = "AI_engineer_3.json"

    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ JSON Ğ Ğ•Ğ—Ğ®ĞœĞ• Ğ’ QDRANT                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ CVParser
    cv_parser = CVParser(collection_name=QDRANT_COLLECTION_NAME)
    
    # ĞŸÑƒÑ‚ÑŒ Ğº JSON Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼
    json_folder = cv_parser.json_cvs_folder
    print(f"ğŸ“ ĞŸĞ°Ğ¿ĞºĞ° Ñ JSON: {json_folder}")
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ JSON Ñ„Ğ°Ğ¹Ğ»Ñ‹
    json_files = load_json_files(json_folder, needing_file=needing_file)
    
    if not json_files:
        print("\nâŒ JSON Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
        return
    
    print(f"ğŸ“‹ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: {len(json_files)}\n")
    
    # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    success_count = 0
    skip_count = 0
    error_count = 0
    
    # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ JSON
    for idx, (json_file, data) in enumerate(json_files, 1):
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(json_files)}] ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°: {json_file.name}")
        print(f"{'='*60}")
        
        try:
            # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ğ² CVOutput
            cv_data = json_to_cv_output(data)
            print(f"ğŸ‘¤ ĞšĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚: {cv_data.full_name}")
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
            print("ğŸ” Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°...")
            searchable_text = cv_parser.create_searchable_text(cv_data)
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸
            dense_vector, sparse_indices, sparse_values = cv_parser.create_embeddings(searchable_text)
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ source_file Ğ¸Ğ· Ğ¸Ğ¼ĞµĞ½Ğ¸ JSON
            source_file = json_file.stem
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Qdrant
            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· JSON ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            full_text = data.get("full_content", searchable_text)
            
            point_id = cv_parser.save_to_qdrant(
                cv_data=cv_data,
                full_text=full_text,
                dense_vector=dense_vector,
                sparse_indices=sparse_indices,
                sparse_values=sparse_values,
                source_file=source_file
            )
            
            print(f"âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ² Qdrant (ID: {point_id})")
            success_count += 1
            
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
            error_count += 1
            continue
    
    # ĞŸĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ sparse Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
    if args.refit_sparse and cv_parser._sparse_corpus:
        print(f"\n{'='*60}")
        print("ğŸ”„ ĞŸĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ sparse Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
        cv_parser.refit_sparse(auto_save=True)
    
    # Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    print(f"\n{'='*60}")
    print("ğŸ“Š Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ¯ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ")
    print(f"{'='*60}")
    print(f"âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾: {success_count}")
    print(f"â­ï¸  ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾: {skip_count}")
    print(f"âŒ ĞÑˆĞ¸Ğ±Ğ¾Ğº: {error_count}")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ² Qdrant
    try:
        collection_info = cv_parser.qdrant_client.get_collection(cv_parser.collection_name)
        print(f"\nâ˜ï¸  Ğ’ÑĞµĞ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Qdrant: {collection_info.points_count}")
    except Exception as e:
        print(f"\nâš ï¸  ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸: {e}")
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾: {success_count}
âŒ ĞÑˆĞ¸Ğ±Ğ¾Ğº: {error_count}
    """)


if __name__ == "__main__":
    main()

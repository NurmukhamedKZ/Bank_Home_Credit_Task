#!/usr/bin/env python3
"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤ –ø–æ–∏—Å–∫–∞: Dense-only, Sparse-only, Hybrid.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python -m app.scripts.compare_modes           # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤
    python -m app.scripts.compare_modes --sparse  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ TF-IDF vs BM25
"""

import sys
from pathlib import Path
import pandas as pd
from datetime import datetime


def compare_search_modes():
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –≤—Å–µ —Ç—Ä–∏ —Ä–µ–∂–∏–º–∞ –ø–æ–∏—Å–∫–∞"""
    from app.services.cv_parser import CVParser
    from app.evaluation.evaluator import CVSearchEvaluator
    from app.core.config import QDRANT_COLLECTION_NAME
    
    print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë              –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ñ–ò–ú–û–í –ü–û–ò–°–ö–ê                         ‚ïë
‚ïë  Dense-only | Sparse-only (TF-IDF) | Hybrid                   ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    parser = CVParser(collection_name=QDRANT_COLLECTION_NAME)
    evaluator = CVSearchEvaluator(parser)
    
    has_sparse = parser._sparse_fitted
    if not has_sparse:
        print("‚ö†Ô∏è  TF-IDF –Ω–µ –æ–±—É—á–µ–Ω!")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python -m app.scripts.load_cvs\n")
    
    # Dense-only
    print("="*70)
    print("1Ô∏è‚É£  –†–ï–ñ–ò–ú: Dense-only (Voyage AI)")
    print("="*70)
    
    df_dense, results_dense = evaluator.evaluate_all(top_k=10, search_mode="dense")
    
    # Sparse-only
    df_sparse = None
    if has_sparse:
        print("\n" + "="*70)
        print("2Ô∏è‚É£  –†–ï–ñ–ò–ú: Sparse-only (TF-IDF)")
        print("="*70)
        
        df_sparse, _ = evaluator.evaluate_all(top_k=10, search_mode="sparse")
    
    # Hybrid
    df_hybrid = None
    if has_sparse:
        print("\n" + "="*70)
        print("3Ô∏è‚É£  –†–ï–ñ–ò–ú: Hybrid Search (Dense + TF-IDF)")
        print("="*70)
        
        df_hybrid, _ = evaluator.evaluate_all(top_k=10, search_mode="hybrid")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print("\n" + "="*70)
    print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*70 + "\n")
    
    metrics_to_compare = ['precision@5', 'recall@10', 'map', 'mrr', 'ndcg@5', 'f1@5']
    
    comparison_data = []
    for metric in metrics_to_compare:
        if metric not in df_dense.columns:
            continue
        
        row = {'–ú–µ—Ç—Ä–∏–∫–∞': metric.upper()}
        
        dense_avg = df_dense[metric].mean()
        row['Dense'] = f"{dense_avg:.3f}"
        
        if df_sparse is not None and metric in df_sparse.columns:
            sparse_avg = df_sparse[metric].mean()
            row['Sparse'] = f"{sparse_avg:.3f}"
            diff = ((sparse_avg - dense_avg) / dense_avg * 100) if dense_avg > 0 else 0
            row['Sparse vs Dense'] = f"{diff:+.1f}%"
        
        if df_hybrid is not None and metric in df_hybrid.columns:
            hybrid_avg = df_hybrid[metric].mean()
            row['Hybrid'] = f"{hybrid_avg:.3f}"
            diff = ((hybrid_avg - dense_avg) / dense_avg * 100) if dense_avg > 0 else 0
            row['Hybrid vs Dense'] = f"{diff:+.1f}%"
        
        comparison_data.append(row)
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    print("\n" + "="*70)
    print("üéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("="*70 + "\n")
    
    avg_dense_map = df_dense['map'].mean()
    map_scores = {'Dense': avg_dense_map}
    
    if df_sparse is not None:
        map_scores['Sparse'] = df_sparse['map'].mean()
    if df_hybrid is not None:
        map_scores['Hybrid'] = df_hybrid['map'].mean()
    
    best_mode = max(map_scores, key=map_scores.get)
    best_score = map_scores[best_mode]
    
    print(f"üìä –°—Ä–µ–¥–Ω–∏–π MAP –ø–æ —Ä–µ–∂–∏–º–∞–º:")
    for mode, score in map_scores.items():
        emoji = "üèÜ" if mode == best_mode else "  "
        print(f"   {emoji} {mode:<15} {score:.3f}")
    
    print(f"\n‚úÖ –õ–£–ß–®–ò–ô –†–ï–ñ–ò–ú: {best_mode} (MAP: {best_score:.3f})")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    output_dir = Path("evaluation_results")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    comparison_df.to_csv(output_dir / f"comparison_all_modes_{timestamp}.csv", index=False)
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: comparison_all_modes_{timestamp}.csv")
    
    return comparison_df


def compare_sparse_methods():
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç TF-IDF –∏ BM25"""
    from app.services.cv_parser import CVParser
    from app.evaluation.evaluator import CVSearchEvaluator
    
    print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë        –°–†–ê–í–ù–ï–ù–ò–ï SPARSE –ú–ï–¢–û–î–û–í: TF-IDF vs BM25               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    results_all = {}
    
    # TF-IDF
    print("="*70)
    print("üî§ TF-IDF –¢–ï–°–¢–´")
    print("="*70 + "\n")
    
    parser_tfidf = CVParser(collection_name=QDRANT_COLLECTION_NAME, sparse_method="tfidf")
    
    if not parser_tfidf._sparse_fitted:
        print("‚ö†Ô∏è  TF-IDF –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    else:
        evaluator_tfidf = CVSearchEvaluator(parser_tfidf)
        
        print("1Ô∏è‚É£  TF-IDF Sparse-only")
        df_tfidf_sparse, _ = evaluator_tfidf.evaluate_all(top_k=10, search_mode="sparse")
        results_all['tfidf_sparse'] = df_tfidf_sparse
        
        print("\n2Ô∏è‚É£  TF-IDF Hybrid")
        df_tfidf_hybrid, _ = evaluator_tfidf.evaluate_all(top_k=10, search_mode="hybrid")
        results_all['tfidf_hybrid'] = df_tfidf_hybrid
    
    # BM25
    print("\n" + "="*70)
    print("üéØ BM25 –¢–ï–°–¢–´")
    print("="*70 + "\n")
    
    parser_bm25 = CVParser(collection_name=QDRANT_COLLECTION_NAME, sparse_method="bm25")
    
    if not parser_bm25._sparse_fitted:
        print("‚ö†Ô∏è  BM25 –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    else:
        evaluator_bm25 = CVSearchEvaluator(parser_bm25)
        
        print("3Ô∏è‚É£  BM25 Sparse-only")
        df_bm25_sparse, _ = evaluator_bm25.evaluate_all(top_k=10, search_mode="sparse")
        results_all['bm25_sparse'] = df_bm25_sparse
        
        print("\n4Ô∏è‚É£  BM25 Hybrid")
        df_bm25_hybrid, _ = evaluator_bm25.evaluate_all(top_k=10, search_mode="hybrid")
        results_all['bm25_hybrid'] = df_bm25_hybrid
    
    if len(results_all) == 0:
        print("\n‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è!")
        return
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print("\n" + "="*70)
    print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*70 + "\n")
    
    metrics_to_compare = ['precision@5', 'recall@10', 'map', 'mrr', 'ndcg@5', 'f1@5']
    
    comparison_data = []
    for metric in metrics_to_compare:
        row = {'–ú–µ—Ç—Ä–∏–∫–∞': metric.upper()}
        
        for method_name, df in results_all.items():
            if df is not None and metric in df.columns:
                row[method_name.replace('_', ' ').title()] = f"{df[metric].mean():.3f}"
        
        comparison_data.append(row)
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))
    
    # –õ—É—á—à–∏–π –º–µ—Ç–æ–¥
    best_map = 0
    best_method = None
    
    for method_name, df in results_all.items():
        if df is not None and 'map' in df.columns:
            map_value = df['map'].mean()
            if map_value > best_map:
                best_map = map_value
                best_method = method_name
    
    if best_method:
        print(f"\nüèÜ –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_method.replace('_', ' ').upper()} (MAP: {best_map:.3f})")
    
    return comparison_df


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤ –ø–æ–∏—Å–∫–∞")
    parser.add_argument("--sparse", action="store_true", help="–°—Ä–∞–≤–Ω–∏—Ç—å TF-IDF vs BM25")
    
    args = parser.parse_args()
    
    if args.sparse:
        compare_sparse_methods()
    else:
        compare_search_modes()


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ML ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ½Ğ° Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸ÑÑ….

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    python -m app.scripts.evaluate_ml_classifier
    python -m app.scripts.evaluate_ml_classifier --model-path models/my_classifier.pkl
"""

import sys
from pathlib import Path
import json
from datetime import datetime
from typing import Dict, List
import pandas as pd


def evaluate_ml_classifier(
    model_path: str = None,
    threshold: float = 0.5,
    save_results: bool = True
):
    """
    ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ML ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸ÑÑ…
    
    Args:
        model_path: ĞŸÑƒÑ‚ÑŒ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (ĞµÑĞ»Ğ¸ None - Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ)
        threshold: ĞŸĞ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (default: 0.5)
        save_results: Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ»Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
    """
    from app.services.cv_parser import CVParser
    from app.evaluation.evaluator import CVSearchEvaluator
    from app.services.ml_classifier import MLClassifier, build_training_data_from_ground_truth
    from app.evaluation.metrics import SearchMetrics
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ĞĞ¦Ğ•ĞĞšĞ ML ĞšĞ›ĞĞ¡Ğ¡Ğ˜Ğ¤Ğ˜ĞšĞĞ¢ĞĞ Ğ                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    from app.core.config import QDRANT_COLLECTION_NAME
    
    print("ğŸš€ Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ CVParser Ğ¸ Evaluator...")
    parser = CVParser(collection_name=QDRANT_COLLECTION_NAME)
    evaluator = CVSearchEvaluator(parser)
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    if model_path and Path(model_path).exists():
        print(f"ğŸ“‚ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {model_path}")
        classifier = MLClassifier.load(model_path)
    else:
        print("ğŸ¤– ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
        
        # ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸
        vacancy_texts, cv_texts, labels = build_training_data_from_ground_truth(
            evaluator,
            negative_ratio=1.5
        )
        
        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
        classifier = MLClassifier(
            model_type='logistic',
            tfidf_max_features=5000,
            tfidf_ngram_range=(1, 2)
        )
        
        classifier.fit(vacancy_texts, cv_texts, labels, validation_split=0.1, verbose=True)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        model_dir = Path("data/models")
        model_dir.mkdir(parents=True, exist_ok=True)
        model_path = model_dir / "ml_classifier_evaluation.pkl"
        classifier.save(model_path)
    
    print(f"\n{'='*70}")
    print("ĞĞ¦Ğ•ĞĞšĞ ĞĞ Ğ’ĞĞšĞĞĞ¡Ğ˜Ğ¯Ğ¥")
    print(f"{'='*70}\n")
    
    results = []
    
    # Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸
    for vacancy_name in evaluator.vacancies.keys():
        print(f"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°: {vacancy_name}...", end=' ')
        
        try:
            vacancy_text = evaluator.vacancies[vacancy_name]
            relevant_cvs = evaluator.ground_truth[vacancy_name]
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ CV
            all_cvs = list(evaluator.cvs_folder.glob("*.txt"))
            
            # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… CV
            cv_scores = []
            
            for cv_path in all_cvs:
                cv_text = cv_path.read_text(encoding='utf-8')
                cv_name = cv_path.stem
                
                # Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
                probability = classifier.predict_proba(vacancy_text, cv_text)
                
                cv_scores.append((cv_name, probability))
            
            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (descending)
            cv_scores.sort(key=lambda x: x[1], reverse=True)
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº
            retrieved_ids = [cv_id for cv_id, _ in cv_scores]
            
            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
            metrics = {}
            
            # Precision@K
            for k in [1, 3, 5, 8, 10]:
                if k <= len(retrieved_ids):
                    metrics[f'precision@{k}'] = SearchMetrics.precision_at_k(
                        relevant_cvs, retrieved_ids, k
                    )
            
            # Recall@K
            for k in [1, 3, 5, 8, 10]:
                if k <= len(retrieved_ids):
                    metrics[f'recall@{k}'] = SearchMetrics.recall_at_k(
                        relevant_cvs, retrieved_ids, k
                    )
            
            # F1@K
            for k in [1, 3, 5, 8, 10]:
                if k <= len(retrieved_ids):
                    metrics[f'f1@{k}'] = SearchMetrics.f1_at_k(
                        relevant_cvs, retrieved_ids, k
                    )
            
            # MAP
            metrics['map'] = SearchMetrics.average_precision(relevant_cvs, retrieved_ids)
            
            # MRR
            metrics['mrr'] = SearchMetrics.mean_reciprocal_rank(relevant_cvs, retrieved_ids)
            
            # NDCG@K
            for k in [1, 3, 5, 8, 10]:
                if k <= len(retrieved_ids):
                    metrics[f'ndcg@{k}'] = SearchMetrics.ndcg_at_k(
                        relevant_cvs, retrieved_ids, k
                    )
            
            # Ğ¢Ğ¾Ğ¿-10 Ñ scores
            top_10 = [
                {
                    'cv_id': cv_id,
                    'score': float(score),
                    'relevant': cv_id in relevant_cvs
                }
                for cv_id, score in cv_scores[:10]
            ]
            
            result = {
                'vacancy': vacancy_name,
                'relevant_count': len(relevant_cvs),
                'metrics': metrics,
                'retrieved': top_10
            }
            
            results.append(result)
            
            print(f"âœ… MAP: {metrics['map']:.3f}")
            
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if len(results) == 0:
        print("\nâš ï¸  ĞĞ•Ğ¢ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ĞĞ’")
        return
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ DataFrame
    rows = []
    for r in results:
        row = {'vacancy': r['vacancy'], 'relevant_count': r['relevant_count']}
        row.update(r['metrics'])
        rows.append(row)
    
    df = pd.DataFrame(rows)
    
    # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
    print(f"\n{'='*70}")
    print("ğŸ“Š Ğ¡Ğ Ğ•Ğ”ĞĞ˜Ğ• ĞœĞ•Ğ¢Ğ Ğ˜ĞšĞ˜")
    print(f"{'='*70}\n")
    
    metric_cols = [col for col in df.columns if col not in ['vacancy', 'relevant_count']]
    if len(metric_cols) > 0:
        summary = df[metric_cols].describe().loc[['mean', 'std', 'min', 'max']]
        print(summary.to_string())
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
    if save_results:
        print(f"\n{'='*70}")
        output_dir = Path("evaluation_results")
        output_dir.mkdir(exist_ok=True, parents=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸
        csv_path = output_dir / f"ml_classifier_{timestamp}.csv"
        df.to_csv(csv_path, index=False)
        print(f"ğŸ’¾ ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {csv_path}")
        
        # JSON Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸
        json_path = output_dir / f"ml_classifier_detailed_{timestamp}.json"
        
        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ sets Ğ² lists
        results_serializable = []
        for r in results:
            r_copy = r.copy()
            results_serializable.append(r_copy)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Ğ”ĞµÑ‚Ğ°Ğ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {json_path}")
        print(f"{'='*70}")
    
    # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
    print(f"\n{'='*70}")
    print("ğŸ“‹ Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ• Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« ĞŸĞ Ğ’ĞĞšĞĞĞ¡Ğ˜Ğ¯Ğœ")
    print(f"{'='*70}\n")
    
    for result in results:
        print(f"\nğŸ¯ Ğ’Ğ°ĞºĞ°Ğ½ÑĞ¸Ñ: {result['vacancy']}")
        print(f"   Ğ ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… CV: {result['relevant_count']}")
        print(f"   MAP: {result['metrics']['map']:.3f}")
        print(f"   MRR: {result['metrics']['mrr']:.3f}")
        print(f"   Precision@5: {result['metrics'].get('precision@5', 0):.3f}")
        print(f"   Recall@10: {result['metrics'].get('recall@10', 0):.3f}")
        
        print(f"\n   Ğ¢Ğ¾Ğ¿-5 Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… CV:")
        for i, item in enumerate(result['retrieved'][:5], 1):
            is_relevant = "âœ…" if item['relevant'] else "âŒ"
            print(f"      {i}. {item['cv_id']:<30} (score: {item['score']:.4f}) {is_relevant}")
    
    print(f"\n{'='*70}")
    print("âœ… ĞĞ¦Ğ•ĞĞšĞ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ")
    print(f"{'='*70}\n")
    
    return df, results


def main():
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ĞÑ†ĞµĞ½ĞºĞ° ML ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°")
    parser.add_argument("--model-path", type=str, help="ĞŸÑƒÑ‚ÑŒ Ğº ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸")
    parser.add_argument("--threshold", type=float, default=0.5, help="ĞŸĞ¾Ñ€Ğ¾Ğ³ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (default: 0.5)")
    parser.add_argument("--no-save", action="store_true", help="ĞĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
    
    args = parser.parse_args()
    
    evaluate_ml_classifier(
        model_path=args.model_path,
        threshold=args.threshold,
        save_results=not args.no_save
    )


if __name__ == "__main__":
    main()

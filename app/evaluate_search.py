"""
–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ —Ä–µ–∑—é–º–µ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ IR (Information Retrieval)
"""

from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
import numpy as np
import pandas as pd
from datetime import datetime
import json

from service.parse_pdf import CVParser


class SearchMetrics:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞"""
    
    @staticmethod
    def precision_at_k(relevant: Set[str], retrieved: List[str], k: int) -> float:
        """Precision@K: –¥–æ–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–æ–ø-K"""
        if k == 0:
            return 0.0
        top_k = set(retrieved[:k])
        return len(top_k & relevant) / k
    
    @staticmethod
    def recall_at_k(relevant: Set[str], retrieved: List[str], k: int) -> float:
        """Recall@K: –¥–æ–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö"""
        if len(relevant) == 0:
            return 0.0
        top_k = set(retrieved[:k])
        return len(top_k & relevant) / len(relevant)
    
    @staticmethod
    def f1_at_k(relevant: Set[str], retrieved: List[str], k: int) -> float:
        """F1-score@K: –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ precision –∏ recall"""
        p = SearchMetrics.precision_at_k(relevant, retrieved, k)
        r = SearchMetrics.recall_at_k(relevant, retrieved, k)
        if p + r == 0:
            return 0.0
        return 2 * (p * r) / (p + r)
    
    @staticmethod
    def average_precision(relevant: Set[str], retrieved: List[str]) -> float:
        """Average Precision: —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if len(relevant) == 0:
            return 0.0
        
        avg_precision = 0.0
        num_relevant = 0
        
        for i, doc_id in enumerate(retrieved, 1):
            if doc_id in relevant:
                num_relevant += 1
                precision_at_i = num_relevant / i
                avg_precision += precision_at_i
        
        return avg_precision / len(relevant)
    
    @staticmethod
    def mean_reciprocal_rank(relevant: Set[str], retrieved: List[str]) -> float:
        """MRR: –ø–æ–∑–∏—Ü–∏—è –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        for i, doc_id in enumerate(retrieved, 1):
            if doc_id in relevant:
                return 1.0 / i
        return 0.0
    
    @staticmethod
    def ndcg_at_k(relevant: Set[str], retrieved: List[str], k: int) -> float:
        """NDCG@K: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –≤—ã–∏–≥—Ä—ã—à"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ª–∏–±–æ 1, –ª–∏–±–æ 0
        dcg = 0.0
        for i, doc_id in enumerate(retrieved[:k], 1):
            if doc_id in relevant:
                dcg += 1.0 / np.log2(i + 1)
        
        # Ideal DCG
        idcg = sum(1.0 / np.log2(i + 1) for i in range(1, min(len(relevant), k) + 1))
        
        return dcg / idcg if idcg > 0 else 0.0


class CVSearchEvaluator:
    """
    –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ CV –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ç—Ä–æ–∏—Ç ground truth –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤
    """
    
    def __init__(
        self,
        parser: CVParser,
        vacancies_folder: str | Path = None,
        cvs_folder: str | Path = None
    ):
        """
        Args:
            parser: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π CVParser —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ Qdrant
            vacancies_folder: –ü–∞–ø–∫–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –≤–∞–∫–∞–Ω—Å–∏–π
            cvs_folder: –ü–∞–ø–∫–∞ —Å —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–º–∏ CV
        """
        self.parser = parser
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
        project_root = Path(__file__).parent.parent
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
        if vacancies_folder is None:
            self.vacancies_folder = project_root / "data" / "vacancy"
        else:
            self.vacancies_folder = Path(vacancies_folder)
        
        if cvs_folder is None:
            self.cvs_folder = project_root / "data" / "Parsed_CVs"
        else:
            self.cvs_folder = Path(cvs_folder)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.vacancies = self._load_vacancies()
        self.ground_truth = self._build_ground_truth()
        
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(self.vacancies)}")
        print(f"üìä Ground truth –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {len(self.ground_truth)} –≤–∞–∫–∞–Ω—Å–∏–π")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
        if len(self.vacancies) == 0:
            print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            print(f"   –ü–∞–ø–∫–∞: {self.vacancies_folder}")
            print(f"   –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.vacancies_folder.exists()}")
            if self.vacancies_folder.exists():
                files = list(self.vacancies_folder.glob("*.txt"))
                print(f"   –§–∞–π–ª–æ–≤ .txt: {len(files)}")
        
        if len(self.ground_truth) == 0:
            print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: CV –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            print(f"   –ü–∞–ø–∫–∞: {self.cvs_folder}")
            print(f"   –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.cvs_folder.exists()}")
            if self.cvs_folder.exists():
                files = list(self.cvs_folder.glob("*.txt"))
                print(f"   –§–∞–π–ª–æ–≤ .txt: {len(files)}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Qdrant
        try:
            collection_info = self.parser.qdrant_client.get_collection(self.parser.collection_name)
            print(f"üìä CV –≤ Qdrant: {collection_info.points_count}")
            
            if collection_info.points_count == 0:
                print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–ª–µ–∫—Ü–∏—è Qdrant –ø—É—Å—Ç–∞!")
                print(f"   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python app/process_cvs.py")
        except Exception as e:
            print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Qdrant: {e}")
            print(f"   –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.parser.collection_name}' –º–æ–∂–µ—Ç –Ω–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å")
    
    def _load_vacancies(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ –ø–∞–ø–∫–∏"""
        vacancies = {}
        
        for file in self.vacancies_folder.glob("*.txt"):
            vacancies[file.stem] = file.read_text(encoding='utf-8')
        
        return vacancies
    
    def _build_ground_truth(self) -> Dict[str, Set[str]]:
        """
        –°—Ç—Ä–æ–∏—Ç ground truth –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤
        
        –õ–æ–≥–∏–∫–∞: –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ "AI_engineer_1" —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è
        –≤—Å–µ CV —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º "AI_engineer_" (AI_engineer_1, AI_engineer_2, etc.)
        
        Returns:
            {vacancy_name: set(relevant_cv_names)}
        """
        ground_truth = {}
        
        for vacancy_name in self.vacancies.keys():
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è (–±–µ–∑ –Ω–æ–º–µ—Ä–∞)
            # "AI_engineer_1" -> "AI_engineer"
            parts = vacancy_name.split('_')
            if len(parts) >= 2 and parts[-1].isdigit():
                base_name = '_'.join(parts[:-1])
            else:
                base_name = vacancy_name
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ CV —Å —Ç–∞–∫–∏–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
            relevant_cvs = {
                f.stem for f in self.cvs_folder.glob(f"{base_name}_*.txt")
            }
            
            ground_truth[vacancy_name] = relevant_cvs
        
        return ground_truth
    
    def search_cvs(
        self,
        query_text: str,
        top_k: int = 10,
        use_hybrid: bool = True
    ) -> List[Tuple[str, float]]:
        """
        –ü–æ–∏—Å–∫ CV —á–µ—Ä–µ–∑ Qdrant
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–≤–∞–∫–∞–Ω—Å–∏—è)
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            use_hybrid: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å hybrid search (dense + sparse)
            
        Returns:
            List[(cv_identifier, score)]
        """
        # –°–æ–∑–¥–∞–µ–º dense embedding
        query_vector = self.parser.dense_model.embed_documents([query_text])[0]
        
        # –ü–æ–∏—Å–∫ –≤ Qdrant
        results = self.parser.qdrant_client.query_points(
            collection_name=self.parser.collection_name,
            query=query_vector,
            using="default",
            limit=top_k,
            with_payload=True,
            with_vectors=False
        )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º source_file –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å ground truth)
        cv_results = []
        for point in results.points:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: source_file (–∏–º—è —Ñ–∞–π–ª–∞) –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∏–ª–∏ full_name –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            cv_identifier = point.payload.get('source_file', point.payload.get('full_name', 'Unknown'))
            score = point.score
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ ID –∏ –∏–º—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            full_name = point.payload.get('full_name', 'Unknown')
            cv_results.append((cv_identifier, score, full_name))
        
        return cv_results
    
    def evaluate_single_vacancy(
        self,
        vacancy_name: str,
        top_k: int = 10,
        use_hybrid: bool = True
    ) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
        
        Returns:
            {
                'vacancy': str,
                'retrieved': List[Tuple[str, float]],
                'relevant': Set[str],
                'metrics': Dict[str, float]
            }
        """
        if vacancy_name not in self.vacancies:
            raise ValueError(f"–í–∞–∫–∞–Ω—Å–∏—è '{vacancy_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        vacancy_text = self.vacancies[vacancy_name]
        relevant_cvs = self.ground_truth[vacancy_name]
        
        # –ü–æ–∏—Å–∫
        retrieved_results = self.search_cvs(vacancy_text, top_k, use_hybrid)
        # retrieved_results —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç (cv_id, score, full_name)
        retrieved_ids = [cv_id for cv_id, _, _ in retrieved_results]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {}
        
        # Precision@K –¥–ª—è —Ä–∞–∑–Ω—ã—Ö K
        for k in [1, 3, 5, 8, 10]:
            if k <= top_k:
                metrics[f'precision@{k}'] = SearchMetrics.precision_at_k(
                    relevant_cvs, retrieved_ids, k
                )
        
        # Recall@K
        for k in [1, 3, 5, 8, 10]:
            if k <= top_k:
                metrics[f'recall@{k}'] = SearchMetrics.recall_at_k(
                    relevant_cvs, retrieved_ids, k
                )
        
        # F1@K
        for k in [1, 3, 5, 8, 10]:
            if k <= top_k:
                metrics[f'f1@{k}'] = SearchMetrics.f1_at_k(
                    relevant_cvs, retrieved_ids, k
                )
        
        # MAP (Mean Average Precision)
        metrics['map'] = SearchMetrics.average_precision(relevant_cvs, retrieved_ids)
        
        # MRR (Mean Reciprocal Rank)
        metrics['mrr'] = SearchMetrics.mean_reciprocal_rank(relevant_cvs, retrieved_ids)
        
        # NDCG@K
        for k in [1, 3, 5, 8, 10]:
            if k <= top_k:
                metrics[f'ndcg@{k}'] = SearchMetrics.ndcg_at_k(
                    relevant_cvs, retrieved_ids, k
                )
        
        return {
            'vacancy': vacancy_name,
            'retrieved': retrieved_results[:5],  # –¢–æ–ø-5 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            'relevant': relevant_cvs,
            'relevant_count': len(relevant_cvs),
            'metrics': metrics
        }
    
    def evaluate_all(self, top_k: int = 10) -> Tuple[pd.DataFrame, List[Dict]]:
        """
        –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –≤–∞–∫–∞–Ω—Å–∏–π
        
        Returns:
            (DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏, —Å–ø–∏—Å–æ–∫ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
        """
        results = []
        
        print(f"\n{'='*60}")
        print("üîç –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ü–û–ò–°–ö–ê")
        print(f"{'='*60}\n")
        
        for vacancy_name in self.vacancies.keys():
            print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {vacancy_name}...", end=' ')
            try:
                result = self.evaluate_single_vacancy(vacancy_name, top_k)
                results.append(result)
                print(f"‚úÖ MAP: {result['metrics']['map']:.3f}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                continue
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
        rows = []
        for r in results:
            row = {'vacancy': r['vacancy'], 'relevant_count': r['relevant_count']}
            row.update(r['metrics'])
            rows.append(row)
        
        df = pd.DataFrame(rows)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if len(df) == 0:
            print(f"\n{'='*60}")
            print("‚ö†Ô∏è  –ù–ï–¢ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –î–õ–Ø –û–¶–ï–ù–ö–ò")
            print(f"{'='*60}\n")
            print("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            print("  1. –ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –ø–∞–ø–∫–µ data/vacancy/")
            print("  2. –ù–µ –Ω–∞–π–¥–µ–Ω—ã CV –≤ –ø–∞–ø–∫–µ data/Parsed_CVs/")
            print("  3. CV –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Qdrant")
            print("\n–†–µ—à–µ–Ω–∏–µ:")
            print("  - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞–ø–∫–∞—Ö")
            print("  - –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python app/process_cvs.py")
            print(f"\n{'='*60}\n")
            return df, results
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\n{'='*60}")
        print("üìä –°–†–ï–î–ù–ò–ï –ú–ï–¢–†–ò–ö–ò –ü–û –í–°–ï–ú –í–ê–ö–ê–ù–°–ò–Ø–ú")
        print(f"{'='*60}\n")
        
        metric_cols = [col for col in df.columns if col not in ['vacancy', 'relevant_count']]
        if len(metric_cols) > 0:
            summary = df[metric_cols].describe().loc[['mean', 'std', 'min', 'max']]
            print(summary.to_string())
        
        print(f"\n{'='*60}\n")
        
        return df, results
    
    def save_results(
        self,
        df: pd.DataFrame,
        results: List[Dict],
        output_dir: str | Path = "evaluation_results"
    ):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        csv_path = output_dir / f"metrics_{timestamp}.csv"
        df.to_csv(csv_path, index=False)
        print(f"üíæ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        
        # JSON —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        json_path = output_dir / f"detailed_{timestamp}.json"
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ lists –¥–ª—è JSON
        results_serializable = []
        for r in results:
            r_copy = r.copy()
            r_copy['relevant'] = list(r['relevant'])
            results_serializable.append(r_copy)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –î–µ—Ç–∞–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_path}")
        
        return csv_path, json_path
    
    def generate_confusion_matrix(self, results: List[Dict]) -> pd.DataFrame:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –ø—É—Ç–∞–Ω–∏—Ü—ã: –∫–∞–∫–∏–µ —Ç–∏–ø—ã CV –Ω–∞—Ö–æ–¥—è—Ç—Å—è –¥–ª—è –∫–∞–∫–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π
        
        Returns:
            DataFrame —Å –º–∞—Ç—Ä–∏—Ü–µ–π –ø—É—Ç–∞–Ω–∏—Ü—ã
        """
        # –¢–∏–ø—ã –ø–æ–∑–∏—Ü–∏–π
        position_types = sorted(set(
            '_'.join(v.split('_')[:-1]) for v in self.vacancies.keys()
        ))
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É
        matrix = pd.DataFrame(
            0,
            index=position_types,
            columns=position_types
        )
        
        for result in results:
            vacancy_name = result['vacancy']
            vacancy_type = '_'.join(vacancy_name.split('_')[:-1])
            
            # –°—á–∏—Ç–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ CV –ø–æ —Ç–∏–ø–∞–º
            for item in result['retrieved']:
                # item –º–æ–∂–µ—Ç –±—ã—Ç—å (cv_id, score, full_name) –∏–ª–∏ (cv_id, score)
                cv_id = item[0] if isinstance(item, (tuple, list)) else item
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø CV –∏–∑ ID
                cv_parts = cv_id.split('_')
                if len(cv_parts) >= 2:
                    cv_type = '_'.join(cv_parts[:-1])
                    if cv_type in position_types:
                        matrix.loc[vacancy_type, cv_type] += 1
        
        return matrix
    
    def print_detailed_results(self, results: List[Dict]):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏"""
        print(f"\n{'='*60}")
        print("üìã –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –í–ê–ö–ê–ù–°–ò–Ø–ú")
        print(f"{'='*60}\n")
        
        for result in results:
            print(f"\nüéØ –í–∞–∫–∞–Ω—Å–∏—è: {result['vacancy']}")
            print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö CV: {result['relevant_count']}")
            print(f"   MAP: {result['metrics']['map']:.3f}")
            print(f"   MRR: {result['metrics']['mrr']:.3f}")
            print(f"   Precision@5: {result['metrics'].get('precision@5', 0):.3f}")
            print(f"   Recall@10: {result['metrics'].get('recall@10', 0):.3f}")
            
            print(f"\n   –¢–æ–ø-5 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö CV:")
            for i, item in enumerate(result['retrieved'], 1):
                # item –º–æ–∂–µ—Ç –±—ã—Ç—å (cv_id, score, full_name) –∏–ª–∏ (cv_id, score)
                if len(item) == 3:
                    cv_id, score, full_name = item
                else:
                    cv_id, score = item
                    full_name = cv_id
                
                is_relevant = "‚úÖ" if cv_id in result['relevant'] else "‚ùå"
                display_name = f"{full_name} [{cv_id}]" if full_name != cv_id else cv_id
                print(f"      {i}. {display_name:<40} (score: {score:.4f}) {is_relevant}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ—Ü–µ–Ω–∫–∏"""
    from service.parse_pdf import CVParser
    
    print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CVParser...")
    parser = CVParser(collection_name="CVs")
    
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ü–µ–Ω—â–∏–∫–∞...")
    evaluator = CVSearchEvaluator(parser)
    
    # –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    df, results = evaluator.evaluate_all(top_k=10)
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    evaluator.print_detailed_results(results)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã
    print(f"\n{'='*60}")
    print("üé≠ –ú–ê–¢–†–ò–¶–ê –ü–£–¢–ê–ù–ò–¶–´ (–Ω–∞–π–¥–µ–Ω–Ω—ã–µ CV –ø–æ —Ç–∏–ø–∞–º)")
    print(f"{'='*60}\n")
    confusion = evaluator.generate_confusion_matrix(results)
    print(confusion.to_string())
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n{'='*60}")
    evaluator.save_results(df, results)
    print(f"{'='*60}\n")
    
    return df, results


if __name__ == "__main__":
    main()

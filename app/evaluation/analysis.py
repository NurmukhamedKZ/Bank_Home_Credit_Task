"""
–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞.
–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import List, Dict
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats


def load_experiment_results(result_file: str | Path) -> Dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏–∑ JSON"""
    with open(result_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def compare_two_experiments(result1: Dict, result2: Dict) -> pd.DataFrame:
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
    metrics1 = result1['metrics_summary']['mean']
    metrics2 = result2['metrics_summary']['mean']
    
    comparison = []
    for metric in metrics1.keys():
        val1 = metrics1[metric]
        val2 = metrics2[metric]
        improvement = ((val2 - val1) / val1 * 100) if val1 > 0 else 0
        
        comparison.append({
            'metric': metric,
            f"{result1['config']['name']}": val1,
            f"{result2['config']['name']}": val2,
            'improvement_%': improvement
        })
    
    return pd.DataFrame(comparison)


def plot_metrics_comparison(results: List[Dict], output_path: str = None):
    """–°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""
    data = []
    for result in results:
        row = {'experiment': result['config']['name']}
        row.update(result['metrics_summary']['mean'])
        data.append(row)
    
    df = pd.DataFrame(data)
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('Comparison of Search Quality Metrics', fontsize=16, fontweight='bold')
    
    metrics = ['precision@5', 'recall@10', 'map', 'mrr', 'ndcg@5', 'f1@5']
    
    for idx, metric in enumerate(metrics):
        if metric not in df.columns:
            continue
        
        row = idx // 3
        col = idx % 3
        ax = axes[row, col]
        
        df.plot(x='experiment', y=metric, kind='bar', ax=ax, legend=False)
        ax.set_title(metric.upper(), fontweight='bold')
        ax.set_xlabel('')
        ax.set_ylabel('Score')
        ax.set_ylim([0, 1.0])
        ax.grid(axis='y', alpha=0.3)
        
        for container in ax.containers:
            ax.bar_label(container, fmt='%.3f', padding=3)
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    else:
        plt.show()
    
    return fig


def statistical_significance_test(results1: List[float], results2: List[float]) -> Dict:
    """–¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–∏–π"""
    t_stat, p_value = stats.ttest_rel(results1, results2)
    
    mean_diff = np.mean(results2) - np.mean(results1)
    pooled_std = np.sqrt((np.std(results1)**2 + np.std(results2)**2) / 2)
    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0
    
    return {
        't_statistic': t_stat,
        'p_value': p_value,
        'significant': p_value < 0.05,
        'cohens_d': cohens_d,
        'effect_size': (
            'small' if abs(cohens_d) < 0.5 else
            'medium' if abs(cohens_d) < 0.8 else
            'large'
        ),
        'mean_improvement': mean_diff
    }


def analyze_per_vacancy_performance(results: List[Dict]) -> pd.DataFrame:
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏"""
    rows = []
    
    for result in results:
        for vacancy_result in result['detailed_results']:
            row = {
                'experiment': result['config']['name'],
                'vacancy': vacancy_result['vacancy']
            }
            row.update(vacancy_result['metrics'])
            rows.append(row)
    
    return pd.DataFrame(rows)


def find_difficult_vacancies(per_vacancy_df: pd.DataFrame, metric: str = 'map') -> pd.DataFrame:
    """–ù–∞—Ö–æ–¥–∏—Ç –≤–∞–∫–∞–Ω—Å–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä—É–¥–Ω–µ–µ –≤—Å–µ–≥–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å"""
    difficulty = per_vacancy_df.groupby('vacancy')[metric].mean().reset_index()
    difficulty['difficulty_score'] = 1 - difficulty[metric]
    difficulty = difficulty.sort_values('difficulty_score', ascending=False)
    
    return difficulty


def generate_full_report(
    results: List[Dict],
    output_dir: str | Path = "analysis_reports"
):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    print("\n" + "="*70)
    print("üìä –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–õ–ù–û–ì–û –û–¢–ß–ï–¢–ê")
    print("="*70 + "\n")
    
    # 1. –û–±—â–µ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    print("1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")
    plot_metrics_comparison(
        results,
        output_path=output_dir / "metrics_comparison.png"
    )
    
    # 2. –ê–Ω–∞–ª–∏–∑ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
    print("2. –ê–Ω–∞–ª–∏–∑ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º...")
    per_vacancy_df = analyze_per_vacancy_performance(results)
    per_vacancy_df.to_csv(output_dir / "per_vacancy_metrics.csv", index=False)
    
    # 3. –°–ª–æ–∂–Ω–æ—Å—Ç—å –≤–∞–∫–∞–Ω—Å–∏–π
    print("3. –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤–∞–∫–∞–Ω—Å–∏–π...")
    difficulty = find_difficult_vacancies(per_vacancy_df)
    difficulty.to_csv(output_dir / "vacancy_difficulty.csv", index=False)
    
    print("\nüìã –°–∞–º—ã–µ —Å–ª–æ–∂–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏:")
    print(difficulty.head().to_string(index=False))
    
    # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å (–µ—Å–ª–∏ >= 2 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)
    if len(results) >= 2:
        print("\n4. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
        
        baseline = results[0]
        baseline_maps = [v['metrics']['map'] for v in baseline['detailed_results']]
        
        sig_tests = []
        for i, result in enumerate(results[1:], 1):
            result_maps = [v['metrics']['map'] for v in result['detailed_results']]
            test_result = statistical_significance_test(baseline_maps, result_maps)
            
            sig_tests.append({
                'comparison': f"{baseline['config']['name']} vs {result['config']['name']}",
                'p_value': test_result['p_value'],
                'significant': test_result['significant'],
                'mean_improvement': test_result['mean_improvement'],
                'cohens_d': test_result['cohens_d'],
                'effect_size': test_result['effect_size']
            })
        
        sig_df = pd.DataFrame(sig_tests)
        sig_df.to_csv(output_dir / "statistical_tests.csv", index=False)
        
        print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤:")
        print(sig_df.to_string(index=False))
    
    # 5. –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print("\n5. –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞...")
    summary_rows = []
    for result in results:
        row = {
            'Experiment': result['config']['name'],
            'Description': result['config']['description']
        }
        metrics = result['metrics_summary']['mean']
        row.update({
            'MAP': f"{metrics['map']:.3f}",
            'P@5': f"{metrics['precision@5']:.3f}",
            'R@10': f"{metrics['recall@10']:.3f}",
            'MRR': f"{metrics['mrr']:.3f}"
        })
        summary_rows.append(row)
    
    summary_df = pd.DataFrame(summary_rows)
    summary_df.to_csv(output_dir / "summary.csv", index=False)
    
    print("\n" + "="*70)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê")
    print("="*70)
    print(summary_df.to_string(index=False))
    
    print("\n" + "="*70)
    print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_dir}")
    print("="*70 + "\n")
    
    return output_dir

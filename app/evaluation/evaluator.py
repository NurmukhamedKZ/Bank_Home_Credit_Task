"""
–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ CV –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º.
"""

from pathlib import Path
from typing import List, Dict, Set, Tuple
import pandas as pd
from datetime import datetime
import json

from qdrant_client import models
from qdrant_client.models import Prefetch

from app.services.cv_parser import CVParser
from app.evaluation.metrics import SearchMetrics


class CVSearchEvaluator:
    """
    –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ CV –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ç—Ä–æ–∏—Ç ground truth –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤
    """
    
    def __init__(
        self,
        parser: CVParser,
        vacancies_folder: str | Path = None,
        cvs_folder: str | Path = None
    ):
        """
        Args:
            parser: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π CVParser —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ Qdrant
            vacancies_folder: –ü–∞–ø–∫–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –≤–∞–∫–∞–Ω—Å–∏–π
            cvs_folder: –ü–∞–ø–∫–∞ —Å —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–º–∏ CV
        """
        self.parser = parser
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
        project_root = Path(__file__).parent.parent.parent
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
        if vacancies_folder is None:
            self.vacancies_folder = project_root / "data" / "vacancy"
        else:
            self.vacancies_folder = Path(vacancies_folder)
        
        if cvs_folder is None:
            self.cvs_folder = project_root / "data" / "Parsed_CVs"
        else:
            self.cvs_folder = Path(cvs_folder)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.vacancies = self._load_vacancies()
        self.ground_truth = self._build_ground_truth()
        
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(self.vacancies)}")
        print(f"üìä Ground truth –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {len(self.ground_truth)} –≤–∞–∫–∞–Ω—Å–∏–π")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
        if len(self.vacancies) == 0:
            print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            print(f"   –ü–∞–ø–∫–∞: {self.vacancies_folder}")
        
        if len(self.ground_truth) == 0:
            print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: CV –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            print(f"   –ü–∞–ø–∫–∞: {self.cvs_folder}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Qdrant
        try:
            collection_info = self.parser.qdrant_client.get_collection(self.parser.collection_name)
            print(f"üìä CV –≤ Qdrant: {collection_info.points_count}")
            
            if collection_info.points_count == 0:
                print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–ª–µ–∫—Ü–∏—è Qdrant –ø—É—Å—Ç–∞!")
        except Exception as e:
            print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Qdrant: {e}")
    
    def _load_vacancies(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ –ø–∞–ø–∫–∏"""
        vacancies = {}
        
        for file in self.vacancies_folder.glob("*.txt"):
            vacancies[file.stem] = file.read_text(encoding='utf-8')
        
        return vacancies
    
    def _build_ground_truth(self) -> Dict[str, Set[str]]:
        """
        –°—Ç—Ä–æ–∏—Ç ground truth –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤
        
        –õ–æ–≥–∏–∫–∞: –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ "AI_engineer_1" —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è
        –≤—Å–µ CV —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º "AI_engineer_" (AI_engineer_1, AI_engineer_2, etc.)
        """
        ground_truth = {}
        
        for vacancy_name in self.vacancies.keys():
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è (–±–µ–∑ –Ω–æ–º–µ—Ä–∞)
            parts = vacancy_name.split('_')
            if len(parts) >= 2 and parts[-1].isdigit():
                base_name = '_'.join(parts[:-1])
            else:
                base_name = vacancy_name
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ CV —Å —Ç–∞–∫–∏–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
            relevant_cvs = {
                f.stem for f in self.cvs_folder.glob(f"{base_name}_*.txt")
            }
            
            ground_truth[vacancy_name] = relevant_cvs
        
        return ground_truth
    
    def search_cvs(
        self,
        query_text: str,
        top_k: int = 10,
        search_mode: str = "hybrid"
    ) -> List[Tuple[str, float, str]]:
        """
        –ü–æ–∏—Å–∫ CV —á–µ—Ä–µ–∑ Qdrant —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–≤–∞–∫–∞–Ω—Å–∏—è)
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            search_mode: –†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ - "dense", "sparse", –∏–ª–∏ "hybrid"
            
        Returns:
            List[(cv_identifier, score, full_name)]
        """
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞
        if search_mode not in ["dense", "sparse", "hybrid"]:
            raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π search_mode: {search_mode}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ sparse –º–µ—Ç–æ–¥–∞
        if search_mode in ["sparse", "hybrid"] and not self.parser._sparse_fitted:
            print(f"   ‚ö†Ô∏è  TF-IDF –Ω–µ –æ–±—É—á–µ–Ω, fallback –Ω–∞ dense-only...")
            search_mode = "dense"
        
        # ========== DENSE-ONLY SEARCH ==========
        if search_mode == "dense":
            print("   üîç Dense-only search (Voyage AI)...")
            
            dense_query = self.parser.dense_model.embed_documents([query_text])[0]
            
            results = self.parser.qdrant_client.query_points(
                collection_name=self.parser.collection_name,
                query=dense_query,
                using="default",
                limit=top_k,
                with_payload=True,
                with_vectors=False
            )
        
        # ========== SPARSE-ONLY SEARCH ==========
        elif search_mode == "sparse":
            print("   üîç Sparse-only search (TF-IDF)...")
            
            sparse_indices, sparse_values = self.parser.create_sparse_query(query_text)
            sparse_query_vector = models.SparseVector(
                indices=sparse_indices,
                values=sparse_values
            )
            
            results = self.parser.qdrant_client.query_points(
                collection_name=self.parser.collection_name,
                query=sparse_query_vector,
                using="sparse",
                limit=top_k,
                with_payload=True,
                with_vectors=False
            )
        
        # ========== HYBRID SEARCH ==========
        elif search_mode == "hybrid":
            print("   üîç Hybrid search (Dense + TF-IDF)...")
            
            dense_query = self.parser.dense_model.embed_documents([query_text])[0]
            sparse_indices, sparse_values = self.parser.create_sparse_query(query_text)
            sparse_query_vector = models.SparseVector(
                indices=sparse_indices,
                values=sparse_values
            )
            
            results = self.parser.qdrant_client.query_points(
                collection_name=self.parser.collection_name,
                prefetch=[
                    Prefetch(
                        query=dense_query,
                        using="default",
                        limit=top_k * 2
                    ),
                    Prefetch(
                        query=sparse_query_vector,
                        using="sparse",
                        limit=top_k * 2
                    )
                ],
                query=models.FusionQuery(fusion=models.Fusion.RRF),
                limit=top_k,
                with_payload=True,
                with_vectors=False
            )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        cv_results = []
        for point in results.points:
            cv_identifier = point.payload.get('source_file', point.payload.get('full_name', 'Unknown'))
            score = point.score
            full_name = point.payload.get('full_name', 'Unknown')
            cv_results.append((cv_identifier, score, full_name))
        
        return cv_results
    
    def evaluate_single_vacancy(
        self,
        vacancy_name: str,
        top_k: int = 10,
        search_mode: str = "hybrid"
    ) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏"""
        if vacancy_name not in self.vacancies:
            raise ValueError(f"–í–∞–∫–∞–Ω—Å–∏—è '{vacancy_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        vacancy_text = self.vacancies[vacancy_name]
        relevant_cvs = self.ground_truth[vacancy_name]
        
        # –ü–æ–∏—Å–∫
        retrieved_results = self.search_cvs(vacancy_text, top_k, search_mode)
        retrieved_ids = [cv_id for cv_id, _, _ in retrieved_results]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {}
        
        # Precision@K
        for k in [1, 3, 5, 8, 10]:
            if k <= top_k:
                metrics[f'precision@{k}'] = SearchMetrics.precision_at_k(
                    relevant_cvs, retrieved_ids, k
                )
        
        # Recall@K
        for k in [1, 3, 5, 8, 10]:
            if k <= top_k:
                metrics[f'recall@{k}'] = SearchMetrics.recall_at_k(
                    relevant_cvs, retrieved_ids, k
                )
        
        # F1@K
        for k in [1, 3, 5, 8, 10]:
            if k <= top_k:
                metrics[f'f1@{k}'] = SearchMetrics.f1_at_k(
                    relevant_cvs, retrieved_ids, k
                )
        
        # MAP
        metrics['map'] = SearchMetrics.average_precision(relevant_cvs, retrieved_ids)
        
        # MRR
        metrics['mrr'] = SearchMetrics.mean_reciprocal_rank(relevant_cvs, retrieved_ids)
        
        # NDCG@K
        for k in [1, 3, 5, 8, 10]:
            if k <= top_k:
                metrics[f'ndcg@{k}'] = SearchMetrics.ndcg_at_k(
                    relevant_cvs, retrieved_ids, k
                )
        
        return {
            'vacancy': vacancy_name,
            'retrieved': retrieved_results[:5],
            'relevant': relevant_cvs,
            'relevant_count': len(relevant_cvs),
            'metrics': metrics
        }
    
    def evaluate_all(self, top_k: int = 10, search_mode: str = "hybrid") -> Tuple[pd.DataFrame, List[Dict]]:
        """–ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –≤–∞–∫–∞–Ω—Å–∏–π"""
        results = []
        
        mode_names = {
            "dense": "Dense-only (Voyage AI)",
            "sparse": "Sparse-only (TF-IDF)",
            "hybrid": "Hybrid (Dense + TF-IDF)"
        }
        mode_display = mode_names.get(search_mode, search_mode)
        
        print(f"\n{'='*60}")
        print(f"üîç –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ü–û–ò–°–ö–ê - {mode_display}")
        print(f"{'='*60}\n")
        
        for vacancy_name in self.vacancies.keys():
            print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {vacancy_name}...", end=' ')
            try:
                result = self.evaluate_single_vacancy(vacancy_name, top_k, search_mode)
                results.append(result)
                print(f"‚úÖ MAP: {result['metrics']['map']:.3f}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                continue
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
        rows = []
        for r in results:
            row = {'vacancy': r['vacancy'], 'relevant_count': r['relevant_count']}
            row.update(r['metrics'])
            rows.append(row)
        
        df = pd.DataFrame(rows)
        
        if len(df) == 0:
            print(f"\n‚ö†Ô∏è  –ù–ï–¢ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –î–õ–Ø –û–¶–ï–ù–ö–ò")
            return df, results
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\n{'='*60}")
        print("üìä –°–†–ï–î–ù–ò–ï –ú–ï–¢–†–ò–ö–ò –ü–û –í–°–ï–ú –í–ê–ö–ê–ù–°–ò–Ø–ú")
        print(f"{'='*60}\n")
        
        metric_cols = [col for col in df.columns if col not in ['vacancy', 'relevant_count']]
        if len(metric_cols) > 0:
            summary = df[metric_cols].describe().loc[['mean', 'std', 'min', 'max']]
            print(summary.to_string())
        
        print(f"\n{'='*60}\n")
        
        return df, results
    
    def save_results(
        self,
        df: pd.DataFrame,
        results: List[Dict],
        output_dir: str | Path = "evaluation_results"
    ):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        csv_path = output_dir / f"metrics_{timestamp}.csv"
        df.to_csv(csv_path, index=False)
        print(f"üíæ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        
        # JSON —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        json_path = output_dir / f"detailed_{timestamp}.json"
        
        results_serializable = []
        for r in results:
            r_copy = r.copy()
            r_copy['relevant'] = list(r['relevant'])
            results_serializable.append(r_copy)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –î–µ—Ç–∞–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_path}")
        
        return csv_path, json_path
    
    def generate_confusion_matrix(self, results: List[Dict]) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –ø—É—Ç–∞–Ω–∏—Ü—ã"""
        position_types = sorted(set(
            '_'.join(v.split('_')[:-1]) for v in self.vacancies.keys()
        ))
        
        matrix = pd.DataFrame(
            0,
            index=position_types,
            columns=position_types
        )
        
        for result in results:
            vacancy_name = result['vacancy']
            vacancy_type = '_'.join(vacancy_name.split('_')[:-1])
            
            for item in result['retrieved']:
                cv_id = item[0] if isinstance(item, (tuple, list)) else item
                
                cv_parts = cv_id.split('_')
                if len(cv_parts) >= 2:
                    cv_type = '_'.join(cv_parts[:-1])
                    if cv_type in position_types:
                        matrix.loc[vacancy_type, cv_type] += 1
        
        return matrix
    
    def print_detailed_results(self, results: List[Dict]):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏"""
        print(f"\n{'='*60}")
        print("üìã –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –í–ê–ö–ê–ù–°–ò–Ø–ú")
        print(f"{'='*60}\n")
        
        for result in results:
            print(f"\nüéØ –í–∞–∫–∞–Ω—Å–∏—è: {result['vacancy']}")
            print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö CV: {result['relevant_count']}")
            print(f"   MAP: {result['metrics']['map']:.3f}")
            print(f"   MRR: {result['metrics']['mrr']:.3f}")
            print(f"   Precision@5: {result['metrics'].get('precision@5', 0):.3f}")
            print(f"   Recall@10: {result['metrics'].get('recall@10', 0):.3f}")
            
            print(f"\n   –¢–æ–ø-5 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö CV:")
            for i, item in enumerate(result['retrieved'], 1):
                if len(item) == 3:
                    cv_id, score, full_name = item
                else:
                    cv_id, score = item
                    full_name = cv_id
                
                is_relevant = "‚úÖ" if cv_id in result['relevant'] else "‚ùå"
                display_name = f"{full_name} [{cv_id}]" if full_name != cv_id else cv_id
                print(f"      {i}. {display_name:<40} (score: {score:.4f}) {is_relevant}")

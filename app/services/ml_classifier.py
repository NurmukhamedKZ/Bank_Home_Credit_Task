"""
ML Classifier - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –±–∞–∑–µ TF-IDF –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è CV —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏.

Supervised learning –ø–æ–¥—Ö–æ–¥:
- –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–≤–∞–∫–∞–Ω—Å–∏—è ‚Üí —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ CV)
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç TF-IDF –≤–µ–∫—Ç–æ—Ä—ã –∫–∞–∫ —Ñ–∏—á–∏
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ (LogisticRegression, SVM, RandomForest)
"""

import pickle
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import pandas as pd

from app.models.cv import CVOutput


class MLClassifier:
    """
    ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç TF-IDF –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –∏
    –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π ML –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
    """
    
    SUPPORTED_MODELS = {
        'logistic': LogisticRegression,
        'svm': SVC,
        'random_forest': RandomForestClassifier
    }
    
    def __init__(
        self,
        model_type: str = 'logistic',
        tfidf_max_features: int = 5000,
        tfidf_ngram_range: Tuple[int, int] = (1, 2),
        model_params: Optional[Dict] = None
    ):
        """
        Args:
            model_type: –¢–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ ('logistic', 'svm', 'random_forest')
            tfidf_max_features: –ú–∞–∫—Å–∏–º—É–º —Ñ–∏—á–µ–π –¥–ª—è TF-IDF
            tfidf_ngram_range: N-–≥—Ä–∞–º–º—ã –¥–ª—è TF-IDF
            model_params: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        if model_type not in self.SUPPORTED_MODELS:
            raise ValueError(f"model_type –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–∏–Ω –∏–∑: {list(self.SUPPORTED_MODELS.keys())}")
        
        self.model_type = model_type
        self.tfidf_max_features = tfidf_max_features
        self.tfidf_ngram_range = tfidf_ngram_range
        
        # TF-IDF –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä
        self.vectorizer = TfidfVectorizer(
            max_features=tfidf_max_features,
            ngram_range=tfidf_ngram_range,
            lowercase=True,
            stop_words='english',
            min_df=2,
            max_df=0.95
        )
        
        # –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        model_class = self.SUPPORTED_MODELS[model_type]
        
        if model_params is None:
            model_params = self._get_default_params(model_type)
        
        self.classifier = model_class(**model_params)
        
        # –§–ª–∞–≥–∏ –æ–±—É—á–µ–Ω–Ω–æ—Å—Ç–∏
        self._vectorizer_fitted = False
        self._classifier_fitted = False
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        self.training_stats = {}
    
    @staticmethod
    def _get_default_params(model_type: str) -> Dict:
        """–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏"""
        defaults = {
            'logistic': {
                'C': 1.0,
                'max_iter': 1000,
                'random_state': 42,
                'class_weight': 'balanced'  # –î–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
            },
            'svm': {
                'C': 1.0,
                'kernel': 'rbf',
                'probability': True,  # –î–ª—è predict_proba
                'random_state': 42,
                'class_weight': 'balanced'
            },
            'random_forest': {
                'n_estimators': 100,
                'max_depth': 10,
                'random_state': 42,
                'class_weight': 'balanced'
            }
        }
        return defaults.get(model_type, {})
    
    def prepare_training_data(
        self,
        vacancy_texts: List[str],
        cv_texts: List[str],
        labels: List[int]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            vacancy_texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –≤–∞–∫–∞–Ω—Å–∏–π
            cv_texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ CV
            labels: –ú–µ—Ç–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0 - –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ, 1 - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ)
            
        Returns:
            (X, y) - —Ñ–∏—á–∏ –∏ –º–µ—Ç–∫–∏
        """
        if len(vacancy_texts) != len(cv_texts) or len(vacancy_texts) != len(labels):
            raise ValueError("–†–∞–∑–º–µ—Ä—ã vacancy_texts, cv_texts –∏ labels –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å")
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤–∞–∫–∞–Ω—Å–∏—é –∏ CV –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        combined_texts = [
            f"{vacancy} [SEP] {cv}"
            for vacancy, cv in zip(vacancy_texts, cv_texts)
        ]
        
        print(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ {len(combined_texts)} –ø–∞—Ä (–≤–∞–∫–∞–Ω—Å–∏—è, CV)...")
        
        # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        if not self._vectorizer_fitted:
            print(f"   üî¢ –û–±—É—á–µ–Ω–∏–µ TF-IDF vectorizer...")
            X = self.vectorizer.fit_transform(combined_texts)
            self._vectorizer_fitted = True
            print(f"   ‚úÖ Vocabulary size: {len(self.vectorizer.vocabulary_)}")
        else:
            X = self.vectorizer.transform(combined_texts)
        
        y = np.array(labels)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        unique, counts = np.unique(y, return_counts=True)
        class_dist = dict(zip(unique, counts))
        print(f"   üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {class_dist}")
        
        return X, y
    
    def fit(
        self,
        vacancy_texts: List[str],
        cv_texts: List[str],
        labels: List[int],
        validation_split: float = 0.2,
        verbose: bool = True
    ):
        """
        –û–±—É—á–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        
        Args:
            vacancy_texts: –¢–µ–∫—Å—Ç—ã –≤–∞–∫–∞–Ω—Å–∏–π
            cv_texts: –¢–µ–∫—Å—Ç—ã CV
            labels: –ú–µ—Ç–∫–∏ (0/1)
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            verbose: –í—ã–≤–æ–¥–∏—Ç—å –ª–∏ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        """
        if verbose:
            print(f"\n{'='*70}")
            print(f"–û–ë–£–ß–ï–ù–ò–ï ML –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê ({self.model_type.upper()})")
            print(f"{'='*70}\n")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = self.prepare_training_data(vacancy_texts, cv_texts, labels)
        
        # Train/validation split
        X_train, X_val, y_train, y_val = train_test_split(
            X, y,
            test_size=validation_split,
            random_state=42,
            stratify=y
        )
        
        if verbose:
            print(f"\nüìö –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
            print(f"   Train: {X_train.shape[0]} samples")
            print(f"   Validation: {X_val.shape[0]} samples")
            print(f"   Features: {X_train.shape[1]}")
        
        # –û–±—É—á–µ–Ω–∏–µ
        if verbose:
            print(f"\nü§ñ –û–±—É—á–µ–Ω–∏–µ {self.model_type} –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        
        self.classifier.fit(X_train, y_train)
        self._classifier_fitted = True
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ train
        train_score = self.classifier.score(X_train, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ validation
        val_score = self.classifier.score(X_val, y_val)
        y_val_pred = self.classifier.predict(X_val)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.training_stats = {
            'train_accuracy': train_score,
            'val_accuracy': val_score,
            'train_size': X_train.shape[0],
            'val_size': X_val.shape[0],
            'n_features': X_train.shape[1]
        }
        
        if verbose:
            print(f"\n{'='*70}")
            print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø")
            print(f"{'='*70}")
            print(f"üìä Train Accuracy: {train_score:.4f}")
            print(f"üìä Validation Accuracy: {val_score:.4f}")
            
            # Classification report
            print(f"\nüìã Classification Report (Validation):")
            print(classification_report(y_val, y_val_pred, target_names=['Not Relevant', 'Relevant']))
            
            # Confusion matrix
            cm = confusion_matrix(y_val, y_val_pred)
            print(f"\nüìä Confusion Matrix:")
            print(f"                  Predicted")
            print(f"                  0    1")
            print(f"Actual    0     {cm[0][0]:4d} {cm[0][1]:4d}")
            print(f"          1     {cm[1][0]:4d} {cm[1][1]:4d}")
            
            # ROC AUC (–µ—Å–ª–∏ –µ—Å—Ç—å predict_proba)
            if hasattr(self.classifier, 'predict_proba'):
                y_val_proba = self.classifier.predict_proba(X_val)[:, 1]
                roc_auc = roc_auc_score(y_val, y_val_proba)
                print(f"\nüìä ROC AUC: {roc_auc:.4f}")
                self.training_stats['roc_auc'] = roc_auc
        
        print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    def predict(
        self,
        vacancy_text: str,
        cv_text: str
    ) -> int:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (0 –∏–ª–∏ 1)
        
        Args:
            vacancy_text: –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏
            cv_text: –¢–µ–∫—Å—Ç CV
            
        Returns:
            0 (–Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ) –∏–ª–∏ 1 (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ)
        """
        if not self._classifier_fitted:
            raise ValueError("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω! –í—ã–∑–æ–≤–∏—Ç–µ fit() —Å–Ω–∞—á–∞–ª–∞.")
        
        combined = f"{vacancy_text} [SEP] {cv_text}"
        X = self.vectorizer.transform([combined])
        
        return int(self.classifier.predict(X)[0])
    
    def predict_proba(
        self,
        vacancy_text: str,
        cv_text: str
    ) -> float:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        
        Args:
            vacancy_text: –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏
            cv_text: –¢–µ–∫—Å—Ç CV
            
        Returns:
            –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0.0 - 1.0)
        """
        if not self._classifier_fitted:
            raise ValueError("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω! –í—ã–∑–æ–≤–∏—Ç–µ fit() —Å–Ω–∞—á–∞–ª–∞.")
        
        if not hasattr(self.classifier, 'predict_proba'):
            raise ValueError(f"{self.model_type} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç predict_proba")
        
        combined = f"{vacancy_text} [SEP] {cv_text}"
        X = self.vectorizer.transform([combined])
        
        return float(self.classifier.predict_proba(X)[0][1])
    
    def predict_batch(
        self,
        vacancy_texts: List[str],
        cv_texts: List[str]
    ) -> List[int]:
        """Batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–∞—Ä"""
        if not self._classifier_fitted:
            raise ValueError("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω!")
        
        combined = [f"{v} [SEP] {c}" for v, c in zip(vacancy_texts, cv_texts)]
        X = self.vectorizer.transform(combined)
        
        return self.classifier.predict(X).tolist()
    
    def predict_proba_batch(
        self,
        vacancy_texts: List[str],
        cv_texts: List[str]
    ) -> List[float]:
        """Batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
        if not self._classifier_fitted:
            raise ValueError("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω!")
        
        if not hasattr(self.classifier, 'predict_proba'):
            raise ValueError(f"{self.model_type} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç predict_proba")
        
        combined = [f"{v} [SEP] {c}" for v, c in zip(vacancy_texts, cv_texts)]
        X = self.vectorizer.transform(combined)
        
        return self.classifier.predict_proba(X)[:, 1].tolist()
    
    def save(self, filepath: str | Path):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –¥–∏—Å–∫"""
        filepath = Path(filepath)
        
        model_data = {
            'model_type': self.model_type,
            'vectorizer': self.vectorizer,
            'classifier': self.classifier,
            'vectorizer_fitted': self._vectorizer_fitted,
            'classifier_fitted': self._classifier_fitted,
            'training_stats': self.training_stats,
            'tfidf_max_features': self.tfidf_max_features,
            'tfidf_ngram_range': self.tfidf_ngram_range
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")
    
    @classmethod
    def load(cls, filepath: str | Path) -> 'MLClassifier':
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –¥–∏—Å–∫–∞"""
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
        
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä
        instance = cls(
            model_type=model_data['model_type'],
            tfidf_max_features=model_data['tfidf_max_features'],
            tfidf_ngram_range=model_data['tfidf_ngram_range']
        )
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        instance.vectorizer = model_data['vectorizer']
        instance.classifier = model_data['classifier']
        instance._vectorizer_fitted = model_data['vectorizer_fitted']
        instance._classifier_fitted = model_data['classifier_fitted']
        instance.training_stats = model_data['training_stats']
        
        print(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
        return instance
    
    def get_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π)
        
        Args:
            top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ñ–∏—á–µ–π
            
        Returns:
            DataFrame —Å —Ñ–∏—á–∞–º–∏ –∏ –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç—å—é
        """
        if not self._classifier_fitted:
            raise ValueError("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω!")
        
        feature_names = self.vectorizer.get_feature_names_out()
        
        if self.model_type == 'logistic':
            # –î–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            importances = self.classifier.coef_[0]
        elif self.model_type == 'random_forest':
            # –î–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ - feature_importances
            importances = self.classifier.feature_importances_
        else:
            raise ValueError(f"{self.model_type} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç feature importance")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é
        indices = np.argsort(np.abs(importances))[::-1][:top_n]
        
        df = pd.DataFrame({
            'feature': feature_names[indices],
            'importance': importances[indices]
        })
        
        return df


def build_training_data_from_ground_truth(
    evaluator,
    negative_ratio: float = 1.0
) -> Tuple[List[str], List[str], List[int]]:
    """
    –°—Ç—Ä–æ–∏—Ç –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –∏–∑ ground truth evaluator'–∞
    
    Args:
        evaluator: CVSearchEvaluator —Å ground truth
        negative_ratio: –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∫ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–º
        
    Returns:
        (vacancy_texts, cv_texts, labels)
    """
    from pathlib import Path
    import random
    
    vacancy_texts = []
    cv_texts = []
    labels = []
    
    print(f"üèóÔ∏è  –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –∏–∑ ground truth...")
    
    # –î–ª—è –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
    for vacancy_name, relevant_cvs in evaluator.ground_truth.items():
        if vacancy_name not in evaluator.vacancies:
            continue
        
        vacancy_text = evaluator.vacancies[vacancy_name]
        
        # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ CV)
        for cv_name in relevant_cvs:
            cv_path = evaluator.cvs_folder / f"{cv_name}.txt"
            if cv_path.exists():
                cv_text = cv_path.read_text(encoding='utf-8')
                
                vacancy_texts.append(vacancy_text)
                cv_texts.append(cv_text)
                labels.append(1)  # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
        
        # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã (–Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ CV)
        all_cvs = set(f.stem for f in evaluator.cvs_folder.glob("*.txt"))
        irrelevant_cvs = all_cvs - relevant_cvs
        
        # –°—ç–º–ø–ª–∏—Ä—É–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        n_negative = int(len(relevant_cvs) * negative_ratio)
        sampled_irrelevant = random.sample(
            list(irrelevant_cvs),
            min(n_negative, len(irrelevant_cvs))
        )
        
        for cv_name in sampled_irrelevant:
            cv_path = evaluator.cvs_folder / f"{cv_name}.txt"
            if cv_path.exists():
                cv_text = cv_path.read_text(encoding='utf-8')
                
                vacancy_texts.append(vacancy_text)
                cv_texts.append(cv_text)
                labels.append(0)  # –ù–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
    
    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(labels)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"      –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {sum(labels)}")
    print(f"      –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {len(labels) - sum(labels)}")
    
    return vacancy_texts, cv_texts, labels


# ==================== –¢–ï–°–¢–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø ====================

def test_ml_classifier():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
    from app.services.cv_parser import CVParser
    from app.evaluation.evaluator import CVSearchEvaluator
    
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         –¢–ï–°–¢ ML –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê (TF-IDF + ML)                 ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    from app.core.config import QDRANT_COLLECTION_NAME
    
    print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CVParser –∏ Evaluator...")
    parser = CVParser(collection_name=QDRANT_COLLECTION_NAME)
    evaluator = CVSearchEvaluator(parser)
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
    vacancy_texts, cv_texts, labels = build_training_data_from_ground_truth(
        evaluator,
        negative_ratio=1.5  # 1.5 –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –Ω–∞ 1 –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    classifier = MLClassifier(
        model_type='logistic',
        tfidf_max_features=5000,
        tfidf_ngram_range=(1, 2)
    )
    
    classifier.fit(vacancy_texts, cv_texts, labels, validation_split=0.2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_path = Path("data/models/ml_classifier_logistic.pkl")
    model_path.parent.mkdir(parents=True, exist_ok=True)
    classifier.save(model_path)
    
    # –¢–µ—Å—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print(f"\n{'='*70}")
    print("–¢–ï–°–¢ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø")
    print(f"{'='*70}\n")
    
    if len(evaluator.vacancies) > 0:
        test_vacancy_name = list(evaluator.vacancies.keys())[0]
        test_vacancy_text = evaluator.vacancies[test_vacancy_name]
        
        print(f"–í–∞–∫–∞–Ω—Å–∏—è: {test_vacancy_name}")
        
        # –¢–æ–ø-5 CV
        all_cvs = list(evaluator.cvs_folder.glob("*.txt"))[:5]
        
        for cv_path in all_cvs:
            cv_text = cv_path.read_text(encoding='utf-8')
            
            prediction = classifier.predict(test_vacancy_text, cv_text)
            probability = classifier.predict_proba(test_vacancy_text, cv_text)
            
            relevant = "‚úÖ –†–ï–õ–ï–í–ê–ù–¢–ù–û" if prediction == 1 else "‚ùå –ù–ï –†–ï–õ–ï–í–ê–ù–¢–ù–û"
            
            print(f"   {cv_path.stem}: {relevant} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {probability:.3f})")
    
    print(f"\n‚úÖ –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù!")


if __name__ == "__main__":
    test_ml_classifier()

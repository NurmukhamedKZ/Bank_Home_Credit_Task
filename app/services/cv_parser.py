"""
CVParser - –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—é–º–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ (PDF, DOCX),
–∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ LLM –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Qdrant.
"""

from pathlib import Path
from typing import List, Optional, Dict
import uuid
import json
import pickle
import re

# LlamaParse –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF
from llama_parse import LlamaParse

# LangChain –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
from qdrant_client import QdrantClient, models

# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
from langchain_voyageai import VoyageAIEmbeddings
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from rank_bm25 import BM25Okapi

# Pydantic –º–æ–¥–µ–ª–∏
from app.models.cv import CVOutput

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
from app.core.config import (
    LLAMA_PARSE_API,
    OPENAI_API_KEY,
    QDRANT_API,
    QDRANT_URL,
    QDRANT_COLLECTION_NAME,
    VOYAGE_API,
    DEFAULT_SPARSE_METHOD,
)


class CVParser:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—é–º–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ (PDF, DOCX),
    –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ LLM –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Qdrant.
    """
    
    def __init__(
        self,
        collection_name: str = None,
        dense_model_name: str = "voyage-4-large",
        dense_output_dim: int = 1024,
        sparse_method: str = None,
        raw_cvs_folder: str | Path = None,
        json_cvs_folder: str | Path = None,
        parsed_cvs_folder: str | Path = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
        
        Args:
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant
            dense_model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è dense embeddings
            dense_output_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å dense –≤–µ–∫—Ç–æ—Ä–æ–≤
            sparse_method: –ú–µ—Ç–æ–¥ –¥–ª—è sparse embeddings - "tfidf" –∏–ª–∏ "bm25"
            raw_cvs_folder: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è raw —Ç–µ–∫—Å—Ç–æ–≤ CV (default: data/Raw_CVs)
            json_cvs_folder: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON —Ñ–∞–π–ª–æ–≤ (default: data/CV_JSONs)
            parsed_cvs_folder: –ü–∞–ø–∫–∞ –¥–ª—è parsed —Ç–µ–∫—Å—Ç–æ–≤ (default: data/Parsed_CVs)
        """
        self.collection_name = collection_name or QDRANT_COLLECTION_NAME
        self.sparse_method = (sparse_method or DEFAULT_SPARSE_METHOD).lower()
        
        if self.sparse_method not in ["tfidf", "bm25"]:
            raise ValueError(f"sparse_method –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'tfidf' –∏–ª–∏ 'bm25', –ø–æ–ª—É—á–µ–Ω–æ: {sparse_method}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞–ø–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        project_root = Path(__file__).parent.parent.parent
        self.raw_cvs_folder = Path(raw_cvs_folder) if raw_cvs_folder else project_root / "data" / "Raw_CVs"
        self.json_cvs_folder = Path(json_cvs_folder) if json_cvs_folder else project_root / "data" / "CV_JSONs"
        self.parsed_cvs_folder = Path(parsed_cvs_folder) if parsed_cvs_folder else project_root / "data" / "Parsed_CVs"
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        self.raw_cvs_folder.mkdir(parents=True, exist_ok=True)
        self.json_cvs_folder.mkdir(parents=True, exist_ok=True)
        self.parsed_cvs_folder.mkdir(parents=True, exist_ok=True)
        
        print(f"üìÅ Raw CVs folder: {self.raw_cvs_folder}")
        print(f"üìÅ JSON CVs folder: {self.json_cvs_folder}")
        print(f"üìÅ Parsed CVs folder: {self.parsed_cvs_folder}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        self.pdf_parser = LlamaParse(
            api_key=LLAMA_PARSE_API,
            parse_mode="parse_page_with_llm",
            result_type="markdown",
            high_res_ocr=True,
        )
        
        # LLM –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            api_key=OPENAI_API_KEY,
            temperature=0
        )
        
        self.structured_llm = self.llm.with_structured_output(CVOutput)
        
        # System prompt –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ CV
        self.system_prompt = """
You are an expert technical recruiter and CV parser.
Your task is to extract structured data from the provided resume text.

CRITICAL RULES:
1. Be precise with dates and names.
2. If a specific field is missing, leave it as None or an empty list.
3. For 'work_history', try to split distinct roles even if they are in the same company.
4. Extract ALL technical skills mentioned.
5. In 'total_experience_months', calculate the sum of all work durations.
"""
        
        # –°–æ–∑–¥–∞–µ–º prompt –¥–ª—è LLM
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            ("user", "Resume:\n\n{text}")
        ])
        
        # –¶–µ–ø–æ—á–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.chain = self.prompt | self.structured_llm
        
        # –ú–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.dense_model = VoyageAIEmbeddings(
            voyage_api_key=VOYAGE_API,
            model=dense_model_name,
            output_dimension=dense_output_dim
        )
        
        # Sparse embeddings: TF-IDF –∏–ª–∏ BM25
        if self.sparse_method == "tfidf":
            self.sparse_model = TfidfVectorizer(
                max_features=10000,
                ngram_range=(1, 2),
                min_df=1,
                sublinear_tf=True,
                lowercase=True,
                stop_words='english'
            )
            self._use_bm25 = False
        else:  # bm25
            self.sparse_model = None  # BM25 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
            self._use_bm25 = True
            self._bm25_tokenizer = self._default_tokenizer
        
        # –§–ª–∞–≥ –æ–±—É—á–µ–Ω–Ω–æ—Å—Ç–∏
        self._sparse_fitted = False
        # –ö–æ—Ä–ø—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self._sparse_corpus = []
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å –¥–ª—è BM25
        self._tokenized_corpus = []
        
        # –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        model_filename = f"{self.sparse_method}_{collection_name}.pkl"
        self.sparse_model_path = project_root / "data" / "models" / model_filename
        self.sparse_model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if self.sparse_model_path.exists():
            self.load_sparse_model()
            print(f"‚úÖ {self.sparse_method.upper()} –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {self.sparse_model_path.name}")
        
        # Qdrant –∫–ª–∏–µ–Ω—Ç
        self.qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        self._ensure_collection(dense_output_dim)
    
    @staticmethod
    def _default_tokenizer(text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è BM25"""
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)
        return tokens
    
    def _ensure_collection(self, vector_size: int):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ Qdrant –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
        if not self.qdrant_client.collection_exists(self.collection_name):
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config={
                    "default": models.VectorParams(
                        size=vector_size,
                        distance=models.Distance.COSINE
                    )
                },
                sparse_vectors_config={
                    "sparse": models.SparseVectorParams(
                        index=models.SparseIndexParams(on_disk=True)
                    )
                }
            )
            print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —Å–æ–∑–¥–∞–Ω–∞")
    
    def parse_pdf(self, file_path: str | Path) -> str:
        """
        –ü–∞—Ä—Å–∏—Ç PDF —Ñ–∞–π–ª –≤ —Ç–µ–∫—Å—Ç
        
        Args:
            file_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            
        Returns:
            –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        
        print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ PDF: {file_path.name}")
        parsed_documents = self.pdf_parser.load_data(str(file_path))
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        full_text = "\n\n".join([doc.text for doc in parsed_documents])
        
        print(f"‚úÖ PDF —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω ({len(parsed_documents)} —Å—Ç—Ä–∞–Ω–∏—Ü)")
        return full_text
    
    def parse_docx(self, file_path: str | Path) -> str:
        """
        –ü–∞—Ä—Å–∏—Ç DOCX —Ñ–∞–π–ª –≤ —Ç–µ–∫—Å—Ç
        
        Args:
            file_path: –ü—É—Ç—å –∫ DOCX —Ñ–∞–π–ª—É
            
        Returns:
            –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
        """
        raise NotImplementedError("DOCX –ø–∞—Ä—Å–∏–Ω–≥ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –ø–æ–∑–∂–µ")
    
    def parse_file(self, file_path: str | Path) -> str:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –ø–∞—Ä—Å–∏—Ç –µ–≥–æ
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
        """
        file_path = Path(file_path)
        suffix = file_path.suffix.lower()
        
        if suffix == '.pdf':
            return self.parse_pdf(file_path)
        elif suffix in ['.docx', '.doc']:
            return self.parse_docx(file_path)
        elif suffix == '.txt':
            return file_path.read_text(encoding='utf-8')
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {suffix}")
    
    def extract_cv_data(self, text: str) -> CVOutput:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ —á–µ—Ä–µ–∑ LLM
        
        Args:
            text: –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
            
        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ CV
        """
        print("ü§ñ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ LLM...")
        cv_data = self.chain.invoke({"text": text})
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã: {cv_data.full_name}")
        return cv_data
    
    def create_searchable_text(self, cv: CVOutput) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            cv: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ CV
            
        Returns:
            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        """
        main_info = f"Candidate for {cv.work_history[0].role if cv.work_history else 'Professional'}. "
        skills = f"Main Skills: {', '.join(cv.skills)}. "
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –º–µ—Å—Ç —Ä–∞–±–æ—Ç—ã
        exp_descriptions = []
        for work in cv.work_history:
            exp_descriptions.append(f"{work.role} at {work.company}: {work.description}")
        
        experience_text = " Experience summary: " + " | ".join(exp_descriptions)
        
        search_text = main_info + skills + cv.summary + experience_text
        return search_text.lower()
    
    def create_embeddings(self, text: str) -> tuple[List[float], List[int], List[float]]:
        """
        –°–æ–∑–¥–∞–µ—Ç dense –∏ sparse —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            
        Returns:
            Tuple (dense_vector, sparse_indices, sparse_values)
        """
        print(f"üî¢ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({self.sparse_method.upper()})...")
        
        # Dense embedding
        dense_vector = self.dense_model.embed_documents([text])[0]
        
        # Sparse embedding
        if self._use_bm25:
            sparse_indices, sparse_values = self._create_bm25_embedding(text)
        else:
            sparse_indices, sparse_values = self._create_tfidf_embedding(text)
        
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã ({self.sparse_method.upper()}: {len(sparse_indices)} –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
        return dense_vector, sparse_indices, sparse_values
    
    def _create_tfidf_embedding(self, text: str) -> tuple[List[int], List[float]]:
        """–°–æ–∑–¥–∞–µ—Ç TF-IDF sparse embedding"""
        if not self._sparse_fitted:
            self._sparse_corpus.append(text)
            self.sparse_model.fit(self._sparse_corpus)
            self._sparse_fitted = True
            print(f"   üìö TF-IDF –æ–±—É—á–µ–Ω –Ω–∞ {len(self._sparse_corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")
            self.save_sparse_model()
        else:
            self._sparse_corpus.append(text)
        
        tfidf_vector = self.sparse_model.transform([text])
        tfidf_coo = tfidf_vector.tocoo()
        
        sparse_indices = tfidf_coo.col.tolist()
        sparse_values = tfidf_coo.data.tolist()
        
        return sparse_indices, sparse_values
    
    def _create_bm25_embedding(self, text: str) -> tuple[List[int], List[float]]:
        """–°–æ–∑–¥–∞–µ—Ç BM25 sparse embedding"""
        tokens = self._bm25_tokenizer(text)
        
        if not self._sparse_fitted:
            self._sparse_corpus.append(text)
            self._tokenized_corpus.append(tokens)
            self.sparse_model = BM25Okapi(self._tokenized_corpus)
            self._sparse_fitted = True
            print(f"   üìö BM25 –æ–±—É—á–µ–Ω –Ω–∞ {len(self._tokenized_corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")
            self.save_sparse_model()
        else:
            self._sparse_corpus.append(text)
            self._tokenized_corpus.append(tokens)
        
        scores = self.sparse_model.get_scores(tokens)
        
        sparse_indices = []
        sparse_values = []
        
        all_tokens = []
        for doc_tokens in self._tokenized_corpus:
            all_tokens.extend(doc_tokens)
        vocab = {token: idx for idx, token in enumerate(sorted(set(all_tokens)))}
        
        token_counts = {}
        for token in tokens:
            if token in vocab:
                token_counts[token] = token_counts.get(token, 0) + 1
        
        for token, count in token_counts.items():
            if token in vocab:
                sparse_indices.append(vocab[token])
                sparse_values.append(float(count))
        
        return sparse_indices, sparse_values
    
    def refit_sparse(self, auto_save: bool = True):
        """
        –ü–µ—Ä–µ–æ–±—É—á–∞–µ—Ç sparse –º–æ–¥–µ–ª—å (TF-IDF –∏–ª–∏ BM25) –Ω–∞ –≤—Å–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ
        """
        if len(self._sparse_corpus) == 0:
            print("‚ö†Ô∏è  –ö–æ—Ä–ø—É—Å –ø—É—Å—Ç, –Ω–µ—á–µ–≥–æ –æ–±—É—á–∞—Ç—å")
            return
        
        print(f"üîÑ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ {self.sparse_method.upper()} –Ω–∞ {len(self._sparse_corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...")
        
        if self._use_bm25:
            self._tokenized_corpus = [self._bm25_tokenizer(text) for text in self._sparse_corpus]
            self.sparse_model = BM25Okapi(self._tokenized_corpus)
        else:
            self.sparse_model.fit(self._sparse_corpus)
        
        self._sparse_fitted = True
        print(f"‚úÖ {self.sparse_method.upper()} –ø–µ—Ä–µ–æ–±—É—á–µ–Ω")
        
        if auto_save:
            self.save_sparse_model()
    
    def refit_tfidf(self, auto_save: bool = True):
        """–ê–ª–∏–∞—Å –¥–ª—è refit_sparse (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"""
        self.refit_sparse(auto_save)
    
    def save_sparse_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é sparse –º–æ–¥–µ–ª—å –∏ –∫–æ—Ä–ø—É—Å –Ω–∞ –¥–∏—Å–∫"""
        if not self._sparse_fitted:
            print(f"‚ö†Ô∏è  {self.sparse_method.upper()} –Ω–µ –æ–±—É—á–µ–Ω, –Ω–µ—á–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å")
            return
        
        model_data = {
            'sparse_model': self.sparse_model,
            'corpus': self._sparse_corpus,
            'tokenized_corpus': self._tokenized_corpus if self._use_bm25 else None,
            'fitted': self._sparse_fitted,
            'method': self.sparse_method,
            'use_bm25': self._use_bm25
        }
        
        if not self._use_bm25 and hasattr(self.sparse_model, 'vocabulary_'):
            model_data['vocabulary_size'] = len(self.sparse_model.vocabulary_)
        elif self._use_bm25:
            all_tokens = set()
            for tokens in self._tokenized_corpus:
                all_tokens.update(tokens)
            model_data['vocabulary_size'] = len(all_tokens)
        
        with open(self.sparse_model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        vocab_size = model_data.get('vocabulary_size', 0)
        print(f"üíæ {self.sparse_method.upper()} –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.sparse_model_path.name}")
        print(f"   üìä –°–ª–æ–≤–∞—Ä—å: {vocab_size} —Å–ª–æ–≤, –ö–æ—Ä–ø—É—Å: {len(self._sparse_corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def load_sparse_model(self) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é sparse –º–æ–¥–µ–ª—å —Å –¥–∏—Å–∫–∞"""
        if not self.sparse_model_path.exists():
            return False
        
        try:
            with open(self.sparse_model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            saved_method = model_data.get('method', 'tfidf')
            if saved_method != self.sparse_method:
                print(f"‚ö†Ô∏è  –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ '{saved_method}', –æ–∂–∏–¥–∞–µ—Ç—Å—è '{self.sparse_method}'")
                return False
            
            self.sparse_model = model_data['sparse_model']
            self._sparse_corpus = model_data['corpus']
            self._sparse_fitted = model_data['fitted']
            
            if self._use_bm25:
                self._tokenized_corpus = model_data.get('tokenized_corpus', [])
            
            vocab_size = model_data.get('vocabulary_size', 0)
            print(f"üìÇ {self.sparse_method.upper()} –∑–∞–≥—Ä—É–∂–µ–Ω: {vocab_size} —Å–ª–æ–≤, {len(self._sparse_corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {self.sparse_method.upper()} –º–æ–¥–µ–ª–∏: {e}")
            self._sparse_fitted = False
            self._sparse_corpus = []
            self._tokenized_corpus = []
            return False
    
    def save_tfidf_model(self):
        """–ê–ª–∏–∞—Å –¥–ª—è save_sparse_model (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"""
        self.save_sparse_model()
    
    def load_tfidf_model(self) -> bool:
        """–ê–ª–∏–∞—Å –¥–ª—è load_sparse_model (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"""
        return self.load_sparse_model()
    
    def create_sparse_query(self, query_text: str) -> tuple[List[int], List[float]]:
        """–°–æ–∑–¥–∞–µ—Ç sparse query –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        if not self._sparse_fitted:
            raise ValueError(f"{self.sparse_method.upper()} –Ω–µ –æ–±—É—á–µ–Ω!")
        
        if self._use_bm25:
            return self._create_bm25_query(query_text)
        else:
            return self._create_tfidf_query(query_text)
    
    def _create_tfidf_query(self, query_text: str) -> tuple[List[int], List[float]]:
        """–°–æ–∑–¥–∞–µ—Ç TF-IDF query –≤–µ–∫—Ç–æ—Ä"""
        query_vector = self.sparse_model.transform([query_text.lower()])
        query_coo = query_vector.tocoo()
        
        return query_coo.col.tolist(), query_coo.data.tolist()
    
    def _create_bm25_query(self, query_text: str) -> tuple[List[int], List[float]]:
        """–°–æ–∑–¥–∞–µ—Ç BM25 query –≤–µ–∫—Ç–æ—Ä"""
        query_tokens = self._bm25_tokenizer(query_text)
        scores = self.sparse_model.get_scores(query_tokens)
        
        all_tokens = []
        for doc_tokens in self._tokenized_corpus:
            all_tokens.extend(doc_tokens)
        vocab = {token: idx for idx, token in enumerate(sorted(set(all_tokens)))}
        
        sparse_indices = []
        sparse_values = []
        
        token_counts = {}
        for token in query_tokens:
            if token in vocab:
                token_counts[token] = token_counts.get(token, 0) + 1
        
        for token, count in token_counts.items():
            if token in vocab:
                sparse_indices.append(vocab[token])
                sparse_values.append(float(count))
        
        return sparse_indices, sparse_values
    
    def cv_to_payload(self, cv: CVOutput, full_text: str, source_file: str = None) -> dict:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç CVOutput –≤ payload –¥–ª—è Qdrant"""
        work_history_dicts = [
            {
                "role": work.role,
                "company": work.company,
                "start_date": work.start_date,
                "end_date": work.end_date,
                "description": work.description,
                "technologies": work.technologies
            }
            for work in cv.work_history
        ]
        
        education_dicts = [
            {
                "institution": edu.institution,
                "degree": edu.degree,
                "year": edu.year
            }
            for edu in cv.education
        ]
        
        return {
            "full_content": full_text,
            "full_name": cv.full_name,
            "email": cv.email,
            "phone": cv.phone,
            "links": cv.links,
            "location": cv.location,
            "summary": cv.summary,
            "total_experience_months": cv.total_experience_months,
            "work_history": work_history_dicts,
            "education": education_dicts,
            "skills": cv.skills,
            "languages": cv.languages,
            "source_file": source_file
        }
    
    def save_to_qdrant(
        self,
        cv_data: CVOutput,
        full_text: str,
        dense_vector: List[float],
        sparse_indices: List[int],
        sparse_values: List[float],
        point_id: Optional[str] = None,
        source_file: str = None
    ) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç CV –≤ Qdrant"""
        if point_id is None:
            point_id = str(uuid.uuid4())
        
        payload = self.cv_to_payload(cv_data, full_text, source_file)
        
        point = models.PointStruct(
            id=point_id,
            vector={
                "default": dense_vector,
                "sparse": models.SparseVector(indices=sparse_indices, values=sparse_values)
            },
            payload=payload
        )
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant...")
        self.qdrant_client.upsert(
            collection_name=self.collection_name,
            points=[point],
            wait=True
        )
        
        print(f"‚úÖ CV —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Qdrant (ID: {point_id})")
        return point_id
    
    def save_raw_text(self, full_text: str, original_filename: str) -> Path:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç raw —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ –≤ —Ñ–∞–π–ª"""
        base_name = Path(original_filename).stem
        output_file = self.parsed_cvs_folder / f"{base_name}.txt"
        output_file.write_text(full_text, encoding='utf-8')
        print(f"üíæ Raw —Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file.name}")
        return output_file
    
    def save_json(self, cv_data: CVOutput, original_filename: str) -> Path:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ CV –≤ JSON —Ñ–∞–π–ª"""
        base_name = Path(original_filename).stem
        output_file = self.json_cvs_folder / f"{base_name}.json"
        
        cv_dict = cv_data.model_dump()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(cv_dict, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ JSON —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file.name}")
        return output_file
    
    def process_cv(self, file_path: str | Path) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ CV: –ø–∞—Ä—Å–∏–Ω–≥ -> —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ -> —Ñ–∞–π–ª—ã -> —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ -> Qdrant
        """
        file_path = Path(file_path)
        
        print(f"\n{'='*60}")
        print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {file_path.name}")
        print(f"{'='*60}\n")
        
        # 1. –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª
        full_text = self.parse_file(file_path)
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        cv_data = self.extract_cv_data(full_text)
        
        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã
        print("\nüìù –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        raw_file = self.save_raw_text(full_text, file_path.name)
        json_file = self.save_json(cv_data, file_path.name)
        
        # 4. –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        searchable_text = self.create_searchable_text(cv_data)
        
        # 5. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        dense_vector, sparse_indices, sparse_values = self.create_embeddings(searchable_text)
        
        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
        source_file_stem = file_path.stem
        point_id = self.save_to_qdrant(
            cv_data=cv_data,
            full_text=full_text,
            dense_vector=dense_vector,
            sparse_indices=sparse_indices,
            sparse_values=sparse_values,
            source_file=source_file_stem
        )
        
        print(f"\n{'='*60}")
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÑ Raw —Ç–µ–∫—Å—Ç: {raw_file}")
        print(f"üìã JSON: {json_file}")
        print(f"‚òÅÔ∏è  Qdrant ID: {point_id}")
        print(f"{'='*60}\n")
        
        return {
            "point_id": point_id,
            "full_name": cv_data.full_name,
            "email": cv_data.email,
            "total_experience_months": cv_data.total_experience_months,
            "skills_count": len(cv_data.skills),
            "cv_data": cv_data,
            "raw_file": str(raw_file),
            "json_file": str(json_file)
        }


# Legacy —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def parse_pdf(file_name: str) -> str:
    """Legacy —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF"""
    parser = LlamaParse(
        api_key=LLAMA_PARSE_API,
        parse_mode="parse_page_with_llm",
        result_type="markdown",
        high_res_ocr=True,
    )

    dir_path = Path(__file__)
    pdf_path = dir_path.parent.parent.parent / "data" / "CVs" / file_name

    if pdf_path.exists():
        print(f"–ü–∞—Ä—Å–∏–Ω–≥ PDF: {pdf_path}")
        parsed_documents = parser.load_data(str(pdf_path))
        
        full_cv = ""
        for doc in parsed_documents:
            full_cv += doc.text
        
        return full_cv
    else:
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")

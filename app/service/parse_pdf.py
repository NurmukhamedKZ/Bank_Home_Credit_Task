from dotenv import load_dotenv
from pathlib import Path
from typing import List, Optional, Dict
import os
import uuid
import json
import pickle

# LlamaParse –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF
from llama_parse import LlamaParse

# LangChain –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã CV
from pydantic import BaseModel, Field

# Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
from qdrant_client import QdrantClient, models

# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
from langchain_voyageai import VoyageAIEmbeddings
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

load_dotenv()

# API –∫–ª—é—á–∏
LLAMA_PARSE_API = os.getenv("LLAMA_PARSE_API")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
QDRANT_API = os.getenv("QDRANT_API")
QDRANT_URL = os.getenv("QDRANT_URL")
VOYAGE_API = os.getenv("VOYAGE_API")


# ==================== PYDANTIC –ú–û–î–ï–õ–ò ====================

class WorkExperience(BaseModel):
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã"""
    role: str = Field(description="Job title, e.g. 'Senior Python Developer'")
    company: str = Field(description="Company name")
    start_date: str = Field(description="Start date usually in YYYY-MM format")
    end_date: str = Field(description="End date in YYYY-MM format or 'Present'")
    description: str = Field(description="Short summary of responsibilities and achievements")
    technologies: List[str] = Field(description="Specific tools used in this role")


class Education(BaseModel):
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""
    institution: str
    degree: str = Field(description="Degree, e.g. 'Bachelor in Computer Science'")
    year: str = Field(description="Year of graduation")


class CVOutput(BaseModel):
    """–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ CV"""
    full_name: str = Field(description="Candidate's full name")
    email: Optional[str] = Field(description="Email address")
    phone: Optional[str] = Field(description="Phone number")
    links: List[str] = Field(default_factory=list, description="URLs to LinkedIn, GitHub, Portfolio")
    location: List[str] = Field(default_factory=list, description="Location of the candidate")
    
    summary: str = Field(description="A brief professional summary of the candidate")
    
    total_experience_months: int = Field(description="Total work experience in months")
    
    work_history: List[WorkExperience] = Field(default_factory=list, description="List of work experiences")
    education: List[Education] = Field(default_factory=list)
    
    skills: List[str] = Field(default_factory=list, description="List of technical/hard skills")
    languages: List[str] = Field(default_factory=list, description="Languages spoken and proficiency level")


# ==================== –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° ====================

class CVParser:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—é–º–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ (PDF, DOCX),
    –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ LLM –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Qdrant.
    """
    
    def __init__(
        self,
        collection_name: str = "CVs",
        dense_model_name: str = "voyage-4-large",
        dense_output_dim: int = 1024,
        raw_cvs_folder: str | Path = None,
        json_cvs_folder: str | Path = None,
        parsed_cvs_folder: str | Path = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
        
        Args:
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant
            dense_model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è dense embeddings
            dense_output_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å dense –≤–µ–∫—Ç–æ—Ä–æ–≤
            raw_cvs_folder: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è raw —Ç–µ–∫—Å—Ç–æ–≤ CV (default: data/Raw_CVs)
            json_cvs_folder: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON —Ñ–∞–π–ª–æ–≤ (default: data/CV_JSONs)
        """
        self.collection_name = collection_name
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞–ø–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        project_root = Path(__file__).parent.parent.parent
        self.raw_cvs_folder = Path(raw_cvs_folder) if raw_cvs_folder else project_root / "data" / "Raw_CVs"
        self.json_cvs_folder = Path(json_cvs_folder) if json_cvs_folder else project_root / "data" / "CV_JSONs"
        self.parsed_cvs_folder = Path(parsed_cvs_folder) if parsed_cvs_folder else project_root / "data" / "Parsed_CVs"
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        self.raw_cvs_folder.mkdir(parents=True, exist_ok=True)
        self.json_cvs_folder.mkdir(parents=True, exist_ok=True)
        self.parsed_cvs_folder.mkdir(parents=True, exist_ok=True)
        
        print(f"üìÅ Raw CVs folder: {self.raw_cvs_folder}")
        print(f"üìÅ JSON CVs folder: {self.json_cvs_folder}")
        print(f"üìÅ Parsed CVs folder: {self.parsed_cvs_folder}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        self.pdf_parser = LlamaParse(
            api_key=LLAMA_PARSE_API,
            parse_mode="parse_page_with_llm",
            result_type="markdown",
            high_res_ocr=True,
        )
        
        # LLM –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            api_key=OPENAI_API_KEY,
            temperature=0
        )
        
        self.structured_llm = self.llm.with_structured_output(CVOutput)
        
        # System prompt –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ CV
        self.system_prompt = """
You are an expert technical recruiter and CV parser.
Your task is to extract structured data from the provided resume text.

CRITICAL RULES:
1. Be precise with dates and names.
2. If a specific field is missing, leave it as None or an empty list.
3. For 'work_history', try to split distinct roles even if they are in the same company.
4. Extract ALL technical skills mentioned.
5. In 'total_experience_months', calculate the sum of all work durations.
"""
        
        # –°–æ–∑–¥–∞–µ–º prompt –¥–ª—è LLM
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            ("user", "Resume:\n\n{text}")
        ])
        
        # –¶–µ–ø–æ—á–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.chain = self.prompt | self.structured_llm
        
        # –ú–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.dense_model = VoyageAIEmbeddings(
            voyage_api_key=VOYAGE_API,
            model=dense_model_name,
            output_dimension=dense_output_dim
        )
        
        # TF-IDF –¥–ª—è sparse embeddings
        self.sparse_model = TfidfVectorizer(
            max_features=10000,  # –ú–∞–∫—Å–∏–º—É–º 10k –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
            ngram_range=(1, 2),  # Uni-grams –∏ bi-grams
            min_df=1,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            sublinear_tf=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫—É—é —à–∫–∞–ª—É –¥–ª—è TF
            lowercase=True,  # –ü—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
            stop_words='english'  # –£–¥–∞–ª—è—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä—É—Å—Å–∫–∏–µ)
        )
        
        # –§–ª–∞–≥, –±—ã–ª –ª–∏ –æ–±—É—á–µ–Ω TF-IDF
        self._tfidf_fitted = False
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è TF-IDF
        self._tfidf_corpus = []
        
        # –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π TF-IDF –º–æ–¥–µ–ª–∏
        self.tfidf_model_path = project_root / "data" / "models" / f"tfidf_{collection_name}.pkl"
        self.tfidf_model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º TF-IDF –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if self.tfidf_model_path.exists():
            self.load_tfidf_model()
            print(f"‚úÖ TF-IDF –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {self.tfidf_model_path.name}")
        
        # Qdrant –∫–ª–∏–µ–Ω—Ç
        self.qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        self._ensure_collection(dense_output_dim)
    
    def _ensure_collection(self, vector_size: int):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ Qdrant –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
        if not self.qdrant_client.collection_exists(self.collection_name):
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config={
                    "default": models.VectorParams(
                        size=vector_size,
                        distance=models.Distance.COSINE
                    )
                },
                sparse_vectors_config={
                    "sparse": models.SparseVectorParams(
                        index=models.SparseIndexParams(on_disk=True)
                    )
                }
            )
            print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —Å–æ–∑–¥–∞–Ω–∞")
    
    def parse_pdf(self, file_path: str | Path) -> str:
        """
        –ü–∞—Ä—Å–∏—Ç PDF —Ñ–∞–π–ª –≤ —Ç–µ–∫—Å—Ç
        
        Args:
            file_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            
        Returns:
            –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        
        print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ PDF: {file_path.name}")
        parsed_documents = self.pdf_parser.load_data(str(file_path))
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        full_text = "\n\n".join([doc.text for doc in parsed_documents])
        
        print(f"‚úÖ PDF —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω ({len(parsed_documents)} —Å—Ç—Ä–∞–Ω–∏—Ü)")
        return full_text
    
    def parse_docx(self, file_path: str | Path) -> str:
        """
        –ü–∞—Ä—Å–∏—Ç DOCX —Ñ–∞–π–ª –≤ —Ç–µ–∫—Å—Ç
        
        Args:
            file_path: –ü—É—Ç—å –∫ DOCX —Ñ–∞–π–ª—É
            
        Returns:
            –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
        """
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ DOCX
        # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å python-docx –∏–ª–∏ LlamaParse
        raise NotImplementedError("DOCX –ø–∞—Ä—Å–∏–Ω–≥ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –ø–æ–∑–∂–µ")
    
    def parse_file(self, file_path: str | Path) -> str:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –ø–∞—Ä—Å–∏—Ç –µ–≥–æ
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
        """
        file_path = Path(file_path)
        suffix = file_path.suffix.lower()
        
        if suffix == '.pdf':
            return self.parse_pdf(file_path)
        elif suffix in ['.docx', '.doc']:
            return self.parse_docx(file_path)
        elif suffix == '.txt':
            return file_path.read_text(encoding='utf-8')
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {suffix}")
    
    def extract_cv_data(self, text: str) -> CVOutput:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ —á–µ—Ä–µ–∑ LLM
        
        Args:
            text: –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
            
        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ CV
        """
        print("ü§ñ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ LLM...")
        cv_data = self.chain.invoke({"text": text})
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã: {cv_data.full_name}")
        return cv_data
    
    def create_searchable_text(self, cv: CVOutput) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            cv: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ CV
            
        Returns:
            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        """
        main_info = f"Candidate for {cv.work_history[0].role if cv.work_history else 'Professional'}. "
        skills = f"Main Skills: {', '.join(cv.skills)}. "
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –º–µ—Å—Ç —Ä–∞–±–æ—Ç—ã
        exp_descriptions = []
        for work in cv.work_history:
            exp_descriptions.append(f"{work.role} at {work.company}: {work.description}")
        
        experience_text = " Experience summary: " + " | ".join(exp_descriptions)
        
        search_text = main_info + skills + cv.summary + experience_text
        return search_text.lower()
    
    def create_embeddings(self, text: str) -> tuple[List[float], List[int], List[float]]:
        """
        –°–æ–∑–¥–∞–µ—Ç dense –∏ sparse —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            
        Returns:
            Tuple (dense_vector, sparse_indices, sparse_values)
        """
        print("üî¢ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        
        # Dense embedding
        dense_vector = self.dense_model.embed_documents([text])[0]
        
        # Sparse embedding —Å TF-IDF
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ –∫–æ—Ä–ø—É—Å –µ—Å–ª–∏ –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω
        if not self._tfidf_fitted:
            self._tfidf_corpus.append(text)
            # –û–±—É—á–∞–µ–º TF-IDF –Ω–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ
            self.sparse_model.fit(self._tfidf_corpus)
            self._tfidf_fitted = True
            print(f"   üìö TF-IDF –æ–±—É—á–µ–Ω –Ω–∞ {len(self._tfidf_corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            self.save_tfidf_model()
        else:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ—Ä–ø—É—Å –¥–ª—è –±—É–¥—É—â–µ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            self._tfidf_corpus.append(text)
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –≤ TF-IDF –≤–µ–∫—Ç–æ—Ä
        tfidf_vector = self.sparse_model.transform([text])
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sparse matrix –≤ –∏–Ω–¥–µ–∫—Å—ã –∏ –∑–Ω–∞—á–µ–Ω–∏—è
        # tfidf_vector —ç—Ç–æ scipy.sparse.csr_matrix
        tfidf_coo = tfidf_vector.tocoo()  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ COO —Ñ–æ—Ä–º–∞—Ç
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–µ–Ω—É–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        sparse_indices = tfidf_coo.col.tolist()  # –ò–Ω–¥–µ–∫—Å—ã —Å—Ç–æ–ª–±—Ü–æ–≤ (—Å–ª–æ–≤)
        sparse_values = tfidf_coo.data.tolist()  # –ó–Ω–∞—á–µ–Ω–∏—è TF-IDF
        
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã (TF-IDF: {len(sparse_indices)} –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
        return dense_vector, sparse_indices, sparse_values
    
    def refit_tfidf(self, auto_save: bool = True):
        """
        –ü–µ—Ä–µ–æ–±—É—á–∞–µ—Ç TF-IDF –Ω–∞ –≤—Å–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ
        –ü–æ–ª–µ–∑–Ω–æ –≤—ã–∑–≤–∞—Ç—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            auto_save: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        """
        if len(self._tfidf_corpus) > 0:
            print(f"üîÑ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ TF-IDF –Ω–∞ {len(self._tfidf_corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...")
            self.sparse_model.fit(self._tfidf_corpus)
            self._tfidf_fitted = True
            print("‚úÖ TF-IDF –ø–µ—Ä–µ–æ–±—É—á–µ–Ω")
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            if auto_save:
                self.save_tfidf_model()
        else:
            print("‚ö†Ô∏è  –ö–æ—Ä–ø—É—Å –ø—É—Å—Ç, –Ω–µ—á–µ–≥–æ –æ–±—É—á–∞—Ç—å")
    
    def save_tfidf_model(self):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é TF-IDF –º–æ–¥–µ–ª—å –∏ –∫–æ—Ä–ø—É—Å –Ω–∞ –¥–∏—Å–∫
        """
        if not self._tfidf_fitted:
            print("‚ö†Ô∏è  TF-IDF –Ω–µ –æ–±—É—á–µ–Ω, –Ω–µ—á–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å")
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –∫–æ—Ä–ø—É—Å –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
        model_data = {
            'sparse_model': self.sparse_model,
            'corpus': self._tfidf_corpus,
            'fitted': self._tfidf_fitted,
            'vocabulary_size': len(self.sparse_model.vocabulary_) if hasattr(self.sparse_model, 'vocabulary_') else 0
        }
        
        with open(self.tfidf_model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        vocab_size = model_data['vocabulary_size']
        print(f"üíæ TF-IDF –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.tfidf_model_path.name}")
        print(f"   üìä –°–ª–æ–≤–∞—Ä—å: {vocab_size} —Å–ª–æ–≤, –ö–æ—Ä–ø—É—Å: {len(self._tfidf_corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def load_tfidf_model(self) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é TF-IDF –º–æ–¥–µ–ª—å —Å –¥–∏—Å–∫–∞
        
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        if not self.tfidf_model_path.exists():
            return False
        
        try:
            with open(self.tfidf_model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            self.sparse_model = model_data['sparse_model']
            self._tfidf_corpus = model_data['corpus']
            self._tfidf_fitted = model_data['fitted']
            
            vocab_size = model_data.get('vocabulary_size', 0)
            print(f"üìÇ TF-IDF –∑–∞–≥—Ä—É–∂–µ–Ω: {vocab_size} —Å–ª–æ–≤, {len(self._tfidf_corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ TF-IDF –º–æ–¥–µ–ª–∏: {e}")
            self._tfidf_fitted = False
            self._tfidf_corpus = []
            return False
    
    def create_sparse_query(self, query_text: str) -> tuple[List[int], List[float]]:
        """
        –°–æ–∑–¥–∞–µ—Ç sparse query –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ (TF-IDF)
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–≤–∞–∫–∞–Ω—Å–∏—è)
            
        Returns:
            Tuple (sparse_indices, sparse_values)
        """
        if not self._tfidf_fitted:
            raise ValueError("TF-IDF –Ω–µ –æ–±—É—á–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç.")
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º query –≤ TF-IDF –≤–µ–∫—Ç–æ—Ä
        query_vector = self.sparse_model.transform([query_text.lower()])
        query_coo = query_vector.tocoo()
        
        sparse_indices = query_coo.col.tolist()
        sparse_values = query_coo.data.tolist()
        
        return sparse_indices, sparse_values
    
    def cv_to_payload(self, cv: CVOutput, full_text: str, source_file: str = None) -> dict:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç CVOutput –≤ payload –¥–ª—è Qdrant
        
        Args:
            cv: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ CV
            full_text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è payload
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä–∏
        work_history_dicts = [
            {
                "role": work.role,
                "company": work.company,
                "start_date": work.start_date,
                "end_date": work.end_date,
                "description": work.description,
                "technologies": work.technologies
            }
            for work in cv.work_history
        ]
        
        education_dicts = [
            {
                "institution": edu.institution,
                "degree": edu.degree,
                "year": edu.year
            }
            for edu in cv.education
        ]
        
        payload = {
            "full_content": full_text,
            "full_name": cv.full_name,
            "email": cv.email,
            "phone": cv.phone,
            "links": cv.links,
            "location": cv.location,
            "summary": cv.summary,
            "total_experience_months": cv.total_experience_months,
            "work_history": work_history_dicts,
            "education": education_dicts,
            "skills": cv.skills,
            "languages": cv.languages,
            "source_file": source_file}
        return payload
    
    def save_to_qdrant(
        self,
        cv_data: CVOutput,
        full_text: str,
        dense_vector: List[float],
        sparse_indices: List[int],
        sparse_values: List[float],
        point_id: Optional[str] = None,
        source_file: str = None
    ) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç CV –≤ Qdrant
        
        Args:
            cv_data: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ CV
            full_text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
            dense_vector: Dense —ç–º–±–µ–¥–¥–∏–Ω–≥
            sparse_indices: –ò–Ω–¥–µ–∫—Å—ã sparse —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            sparse_values: –ó–Ω–∞—á–µ–Ω–∏—è sparse —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            point_id: ID —Ç–æ—á–∫–∏ (–µ—Å–ª–∏ None, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            source_file: –ò–º—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö)
            
        Returns:
            ID —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π —Ç–æ—á–∫–∏
        """
        if point_id is None:
            point_id = str(uuid.uuid4())
        
        payload = self.cv_to_payload(cv_data, full_text, source_file)
        
        point = models.PointStruct(
            id=point_id,
            vector={
                "default": dense_vector,
                "sparse": models.SparseVector(indices=sparse_indices, values=sparse_values)
            },
            payload=payload
        )
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant...")
        self.qdrant_client.upsert(
            collection_name=self.collection_name,
            points=[point],
            wait=True
        )
        
        print(f"‚úÖ CV —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Qdrant (ID: {point_id})")
        return point_id
    
    def save_raw_text(self, full_text: str, original_filename: str) -> Path:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç raw —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ –≤ —Ñ–∞–π–ª
        
        Args:
            full_text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
            original_filename: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ + .txt)
        base_name = Path(original_filename).stem
        output_file = self.parsed_cvs_folder / f"{base_name}.txt"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç
        output_file.write_text(full_text, encoding='utf-8')
        print(f"üíæ Raw —Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file.name}")
        
        return output_file
    
    def save_json(self, cv_data: CVOutput, original_filename: str) -> Path:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ CV –≤ JSON —Ñ–∞–π–ª
        
        Args:
            cv_data: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ CV
            original_filename: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        base_name = Path(original_filename).stem
        output_file = self.json_cvs_folder / f"{base_name}.json"
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Pydantic –º–æ–¥–µ–ª—å –≤ dict
        cv_dict = cv_data.model_dump()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON —Å –∫—Ä–∞—Å–∏–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(cv_dict, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ JSON —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file.name}")
        
        return output_file
    
    def process_cv(self, file_path: str | Path) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ CV: –ø–∞—Ä—Å–∏–Ω–≥ -> —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ -> —Ñ–∞–π–ª—ã -> —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ -> Qdrant
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ä–µ–∑—é–º–µ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        file_path = Path(file_path)
        
        print(f"\n{'='*60}")
        print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {file_path.name}")
        print(f"{'='*60}\n")
        
        # 1. –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª
        full_text = self.parse_file(file_path)
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        cv_data = self.extract_cv_data(full_text)
        
        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã
        print("\nüìù –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        raw_file = self.save_raw_text(full_text, file_path.name)
        json_file = self.save_json(cv_data, file_path.name)
        
        # 4. –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        searchable_text = self.create_searchable_text(cv_data)
        
        # 5. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        dense_vector, sparse_indices, sparse_values = self.create_embeddings(searchable_text)
        
        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant —Å –∏–º–µ–Ω–µ–º –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º stem (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è) –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        source_file_stem = file_path.stem
        point_id = self.save_to_qdrant(
            cv_data=cv_data,
            full_text=full_text,
            dense_vector=dense_vector,
            sparse_indices=sparse_indices,
            sparse_values=sparse_values,
            source_file=source_file_stem
        )
        
        print(f"\n{'='*60}")
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÑ Raw —Ç–µ–∫—Å—Ç: {raw_file}")
        print(f"üìã JSON: {json_file}")
        print(f"‚òÅÔ∏è  Qdrant ID: {point_id}")
        print(f"{'='*60}\n")
        
        return {
            "point_id": point_id,
            "full_name": cv_data.full_name,
            "email": cv_data.email,
            "total_experience_months": cv_data.total_experience_months,
            "skills_count": len(cv_data.skills),
            "cv_data": cv_data,
            "raw_file": str(raw_file),
            "json_file": str(json_file)
        }


# ==================== LEGACY –§–£–ù–ö–¶–ò–Ø (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏) ====================

def parse_pdf(file_name: str) -> str:
    """
    Legacy —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF (–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    
    Args:
        file_name: –ò–º—è —Ñ–∞–π–ª–∞ –≤ –ø–∞–ø–∫–µ data/CVs/
        
    Returns:
        –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
    """
    parser = LlamaParse(
        api_key=LLAMA_PARSE_API,
        parse_mode="parse_page_with_llm",
        result_type="markdown",
        high_res_ocr=True,
    )

    dir_path = Path(__file__)
    pdf_path = dir_path.parent.parent.parent / "data" / "CVs" / file_name

    if pdf_path.exists():
        print(f"–ü–∞—Ä—Å–∏–Ω–≥ PDF: {pdf_path}")
        parsed_documents = parser.load_data(str(pdf_path))
        
        full_cv = ""
        for doc in parsed_documents:
            full_cv += doc.text
        
        return full_cv
    else:
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
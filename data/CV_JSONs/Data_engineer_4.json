{
  "full_name": "Кандидат",
  "email": null,
  "phone": null,
  "links": [],
  "location": [
    "Алматы",
    "Тбилиси"
  ],
  "summary": "Data Engineer с 4+ годами опыта в построении надёжных ETL-процессов, разработке data pipeline’ов, оптимизации SQL-запросов и парсинге сложных форматов (XML, JSON). Имею практический опыт работы с несколькими СУБД (PostgreSQL, ClickHouse, Oracle, Vertica, Impala), уверенно использую Python, Airflow, Spark и другие инструменты. Работала как в корпоративных DWH, так и со стримингом и витринами. Готова брать ответственность, работаю по Agile, быстро осваиваю новые технологии.",
  "total_experience_months": 58,
  "work_history": [
    {
      "role": "Developer Data Engineer",
      "company": "Halyk",
      "start_date": "2025-07",
      "end_date": "Present",
      "description": "Проведение миграции данных организации из старых систем и хранилищ в новые. Сбор, анализ и разработка требований к системам баз данных и хранилищам данных. Тестирование ETL-процессов и работы хранилищ данных. Описание концептуальной модели данных для определения процессов работы с данными и требований к ним. Оптимизация запросов к хранилищам и базам данных для повышения скорости выполнения этих запросов. Разработка стратегий для обеспечения отказоустойчивости и масштабируемости систем.",
      "technologies": [
        "ETL",
        "SQL"
      ]
    },
    {
      "role": "Data engineer",
      "company": "Частная компания Qala AI Ltd",
      "start_date": "2024-04",
      "end_date": "2025-05",
      "description": "Участвовала в разработке и поддержке ETL-процессов под ключ, с учётом требований заказчиков и технических регламентов. Разрабатывала выгрузки и автоматические отчёты для страховых компаний с использованием Python (pandas, openpyxl), формировала Excel-отчёты по заданной структуре и фильтрам. Выполняла очистку, нормализацию и валидацию больших объёмов данных перед загрузкой в хранилище. Работала с геоданными (точки, полигоны, мультиполигоны) в рамках проекта «Умный город»: обрабатывала пространственные данные, взаимодействовала с GeoPandas, Shapely, Folium, участвовала в анализе пространственной доступности объектов и населённых пунктов. Проектировала DAG'и в Apache Airflow, настраивала зависимости и графики выполнения. Интегрировала и трансформировала данные из внешних источников (XML, JSON, REST API). Работала с БД: PostgreSQL (включая PostGIS), ClickHouse, Oracle, Vertica, Impala. Сотрудничала с командами аналитиков и инженеров для обеспечения стабильной поставки и трансформации данных под бизнес-задачи. Обеспечивала контроль качества данных и сопровождение пайплайнов в продакшене.",
      "technologies": [
        "Python",
        "pandas",
        "openpyxl",
        "GeoPandas",
        "Shapely",
        "Folium",
        "Apache Airflow",
        "PostgreSQL",
        "ClickHouse",
        "Oracle",
        "Vertica",
        "Impala",
        "XML",
        "JSON",
        "REST API"
      ]
    },
    {
      "role": "Data engineer",
      "company": "Kaspi Bank, АО",
      "start_date": "2023-11",
      "end_date": "2024-04",
      "description": "Работала с DWH корпоративного уровня: извлекала, агрегировала и структурировала данные для внутренних отчётов и внешних аналитических систем. Оптимизировала SQL-запросы: добавление индексов, партиционирование, рефакторинг CTE, оконных функций. Участвовала в сопровождении бизнес-критичных ETL-джобов, обеспечивая стабильность выполнения и контроль качества данных. Внедрила скрипты автоматической валидации агрегатов и расхождений в отчетах, что снизило количество ручных проверок. Сотрудничала с бизнес-аналитиками для трансляции требований в эффективные архитектурные решения.",
      "technologies": [
        "SQL",
        "ETL"
      ]
    },
    {
      "role": "Специалист сектора BigData",
      "company": "АО КазМунайГаз",
      "start_date": "2021-04",
      "end_date": "2023-11",
      "description": "Разработала пайплайн автоматической миграции и синхронизации данных между несколькими PostgreSQL-окружениями (prod/test/dev). Создала витрины данных на базе Spark SQL для аналитических дашбордов, сократив время извлечения данных в 3 раза. Настраивала Apache Airflow: DAG’и, скоординированные под зависимости между задачами и повторными запусками. Работала с Liquibase для контроля версий схем баз данных в рамках CI/CD процессов (GitLab CI). Внедрила мониторинг пайплайнов через Druid, повысив оперативность реагирования на сбои. Вела документацию, ставила задачи и контролировала прогресс в YouTrack.",
      "technologies": [
        "PostgreSQL",
        "Spark SQL",
        "Apache Airflow",
        "Liquibase",
        "GitLab CI",
        "Druid",
        "YouTrack"
      ]
    }
  ],
  "education": [
    {
      "institution": "Казахстанско-Британский технический университет, Алматы",
      "degree": "Software engineering",
      "year": "2024"
    },
    {
      "institution": "Satbayev University",
      "degree": "Программная инженерия, Информатика и вычислительная техника",
      "year": "2022"
    }
  ],
  "skills": [
    "SQL",
    "Git",
    "PostgreSQL",
    "Анализ данных",
    "Gitlab",
    "Agile",
    "Jupyter Notebook",
    "Apache Airflow",
    "pandas",
    "YouTrack",
    "Python",
    "ORACLE",
    "Clickhouse",
    "Vertica",
    "Liquibase",
    "XML",
    "JSON",
    "API",
    "Pipeline",
    "Apache Spark"
  ],
  "languages": [
    "Русский — Родной",
    "Английский — B1 — Средний"
  ]
}